{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def get_cv_idxs(n, cv_idx=0, val_pct=0.2, seed=42):\n",
    "    \"\"\" Get a list of index values for Validation set from a dataset\n",
    "\n",
    "    Arguments:\n",
    "        n : int, Total number of elements in the data set.\n",
    "        cv_idx : int, starting index [idx_start = cv_idx*int(val_pct*n)]\n",
    "        val_pct : (int, float), validation set percentage\n",
    "        seed : seed value for RandomState\n",
    "\n",
    "    Returns:\n",
    "        list of indexes\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n_val = int(val_pct * n)\n",
    "    idx_start = cv_idx * n_val\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[idx_start:idx_start + n_val]\n",
    "\n",
    "\n",
    "def split_by_idx(idxs, *a):\n",
    "    \"\"\"\n",
    "    Split each array passed as *a, to a pair of arrays like this (elements selected by idxs,  the remaining elements)\n",
    "    This can be used to split multiple arrays containing training data to validation and training set.\n",
    "\n",
    "    :param idxs [int]: list of indexes selected\n",
    "    :param a list: list of np.array, each array should have same amount of elements in the first dimension\n",
    "    :return: list of tuples, each containing a split of corresponding array from *a.\n",
    "            First element of each tuple is an array composed from elements selected by idxs,\n",
    "            second element is an array of remaining elements.\n",
    "    \"\"\"\n",
    "    mask = np.zeros(len(a[0]), dtype=bool)\n",
    "    mask[np.array(idxs)] = True\n",
    "    return [(o[mask], o[~mask]) for o in a]\n",
    "\n",
    "\n",
    "class AutoEncoder(object):\n",
    "\n",
    "    def __init__(self, data, validation_perc=0.2, lr=0.001,\n",
    "                 intermediate_size=1000, encoded_size=100):\n",
    "\n",
    "        # create training dataloader and validation tensor\n",
    "        self.data = data\n",
    "        self.val_idxs = get_cv_idxs(n=data.shape[0], val_pct=validation_perc)\n",
    "        [(self.val, self.train)] = split_by_idx(self.val_idxs, data)\n",
    "        self.dataset = AETrainingData(self.train)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=64, shuffle=True,\n",
    "                                     num_workers=multiprocessing.cpu_count())\n",
    "        #print('datal=',self.dataloader)\n",
    "        self.val = torch.from_numpy(self.val.values).\\\n",
    "            type(torch.FloatTensor).cuda()\n",
    "\n",
    "        # instantiate the encoder and decoder nets\n",
    "        size = data.shape[1]\n",
    "        self.encoder = Encoder(size, intermediate_size, encoded_size).cuda()\n",
    "        self.decoder = Decoder(size, intermediate_size, encoded_size).cuda()\n",
    "\n",
    "        # instantiate the optimizers\n",
    "        self.encoder_optimizer = optim.Adam(\n",
    "            self.encoder.parameters(), lr=lr, weight_decay=1e-8)\n",
    "        self.decoder_optimizer = optim.Adam(\n",
    "            self.decoder.parameters(), lr=lr, weight_decay=1e-8)\n",
    "\n",
    "        # instantiate the loss criterion\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_step(self, input_tensor, target_tensor):\n",
    "        # clear the gradients in the optimizers\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through\n",
    "        encoded_representation = self.encoder(input_tensor)\n",
    "        reconstruction = self.decoder(encoded_representation)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.criterion(reconstruction, target_tensor)\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Step the optimizers to update the model weights\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        # Return the loss value to track training progress\n",
    "        return loss.item()\n",
    "    \n",
    "    def reset(self, train=True):\n",
    "        # due to dropout the network behaves differently in training and\n",
    "        # evaluation modes\n",
    "        if train: self.encoder.train(); self.decoder.train()\n",
    "        else: self.encoder.eval(); self.decoder.eval()\n",
    "\n",
    "    def get_val_loss(self, input_tensor, target_tensor):\n",
    "        self.reset(train=False)\n",
    "        encoded = self.encoder(input_tensor)\n",
    "        decoded = self.decoder(encoded)\n",
    "        loss = self.criterion(decoded, target_tensor)\n",
    "        return loss.item()\n",
    "\n",
    "    def train_loop(self, epochs, print_every_n_batches=20):\n",
    "\n",
    "        # Cycle through epochs\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "            # Cycle through batches\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                #print(i,batch)\n",
    "                \n",
    "                self.reset(train=True)\n",
    "\n",
    "                input_tensor = batch['input'].cuda()\n",
    "                target_tensor = batch['target'].cuda()\n",
    "\n",
    "                loss = self.train_step(input_tensor, target_tensor)\n",
    "\n",
    "                if i % print_every_n_batches == 0 and i != 0:\n",
    "                    #print('i=',i)\n",
    "                    val_loss = self.get_val_loss(self.val, self.val)\n",
    "                    print(f'train loss: {round(loss, 8)} | ' +\n",
    "                          f'validation loss: {round(val_loss, 8)}')\n",
    "                    self.train_losses.append(loss)\n",
    "                    self.val_losses.append(val_loss)\n",
    "\n",
    "    def get_encoded_representations(self):\n",
    "        to_encode = torch.from_numpy(self.data.values).type(\n",
    "            torch.FloatTensor).cuda()\n",
    "        self.reset(train=False)\n",
    "        encodings = self.encoder(to_encode).cpu().data.numpy()\n",
    "        return encodings\n",
    "\n",
    "\n",
    "class AETrainingData(Dataset):\n",
    "    \"\"\"\n",
    "    Format the training dataset to be input into the auto encoder.\n",
    "    Takes in dataframe and converts it to a PyTorch Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_train):\n",
    "        self.x = x_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a example from the data set as a pytorch tensor.\n",
    "        \"\"\"\n",
    "        # Get example/target pair at idx as numpy arrays\n",
    "        x, y = self.x.iloc[idx].values, self.x.iloc[idx].values\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "\n",
    "        # Return pair\n",
    "        return {'input': x, 'target': y}\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size, encoding_size):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, intermediate_size),\n",
    "            nn.BatchNorm1d(intermediate_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(intermediate_size, encoding_size),\n",
    "            nn.BatchNorm1d(encoding_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, intermediate_size, encoding_size):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_size, intermediate_size),\n",
    "            nn.BatchNorm1d(intermediate_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(intermediate_size, output_size),\n",
    "            nn.BatchNorm1d(output_size),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'new_df_matrix.pkl', 'rb') as fh:\n",
    "    new_df1 = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10677, 69878)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf=new_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf=ndf.apply(lambda x: x/x.max(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'tfidf_matrix.pkl', 'rb') as fh:\n",
    "    tfidf_df1 = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10677, 6250)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        float32\n",
       "1        float32\n",
       "2        float32\n",
       "3        float32\n",
       "4        float32\n",
       "          ...   \n",
       "69873    float32\n",
       "69874    float32\n",
       "69875    float32\n",
       "69876    float32\n",
       "69877    float32\n",
       "Length: 69878, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf=ndf.astype(np.float32)\n",
    "ndf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       float32\n",
       "1       float32\n",
       "2       float32\n",
       "3       float32\n",
       "4       float32\n",
       "         ...   \n",
       "6245    float32\n",
       "6246    float32\n",
       "6247    float32\n",
       "6248    float32\n",
       "6249    float32\n",
       "Length: 6250, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df1=tfidf_df1.astype(np.float32)\n",
    "tfidf_df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([ndf, tfidf_df1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10677, 76128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10677, 69878)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10677, 6250)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_1 = AutoEncoder(df_new, validation_perc=0.1, lr=1e-3, intermediate_size=1000, encoded_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "train loss: 0.22829801 | validation loss: 0.23946896\n",
      "train loss: 0.21939072 | validation loss: 0.21922839\n",
      "train loss: 0.21248063 | validation loss: 0.2087384\n",
      "train loss: 0.21228375 | validation loss: 0.20661122\n",
      "train loss: 0.20143591 | validation loss: 0.20064071\n",
      "train loss: 0.19694038 | validation loss: 0.18814877\n",
      "train loss: 0.2013987 | validation loss: 0.18710259\n",
      "Epoch 2/100\n",
      "train loss: 0.19121929 | validation loss: 0.18568847\n",
      "train loss: 0.18705833 | validation loss: 0.18478546\n",
      "train loss: 0.17763861 | validation loss: 0.17054464\n",
      "train loss: 0.18457323 | validation loss: 0.16871428\n",
      "train loss: 0.18359958 | validation loss: 0.16488115\n",
      "train loss: 0.1735363 | validation loss: 0.16179849\n",
      "train loss: 0.16676281 | validation loss: 0.16721609\n",
      "Epoch 3/100\n",
      "train loss: 0.16007428 | validation loss: 0.16024408\n",
      "train loss: 0.15430208 | validation loss: 0.14731973\n",
      "train loss: 0.1504062 | validation loss: 0.14156523\n",
      "train loss: 0.15812506 | validation loss: 0.13968122\n",
      "train loss: 0.1457883 | validation loss: 0.14513233\n",
      "train loss: 0.14705271 | validation loss: 0.13651007\n",
      "train loss: 0.13734144 | validation loss: 0.12888801\n",
      "Epoch 4/100\n",
      "train loss: 0.13540462 | validation loss: 0.13114874\n",
      "train loss: 0.14048748 | validation loss: 0.12866233\n",
      "train loss: 0.13566743 | validation loss: 0.12029094\n",
      "train loss: 0.13012624 | validation loss: 0.12946281\n",
      "train loss: 0.12812932 | validation loss: 0.11397768\n",
      "train loss: 0.12297547 | validation loss: 0.11779495\n",
      "train loss: 0.12007283 | validation loss: 0.11239645\n",
      "Epoch 5/100\n",
      "train loss: 0.12140755 | validation loss: 0.10409873\n",
      "train loss: 0.116561 | validation loss: 0.10414079\n",
      "train loss: 0.11407967 | validation loss: 0.10069896\n",
      "train loss: 0.1100907 | validation loss: 0.11090229\n",
      "train loss: 0.11588249 | validation loss: 0.10788937\n",
      "train loss: 0.10853345 | validation loss: 0.10323361\n",
      "train loss: 0.10275082 | validation loss: 0.10121191\n",
      "Epoch 6/100\n",
      "train loss: 0.10482389 | validation loss: 0.09860021\n",
      "train loss: 0.10556021 | validation loss: 0.09137651\n",
      "train loss: 0.11235933 | validation loss: 0.09121411\n",
      "train loss: 0.09371687 | validation loss: 0.09181093\n",
      "train loss: 0.10581028 | validation loss: 0.08642354\n",
      "train loss: 0.09780497 | validation loss: 0.08928296\n",
      "train loss: 0.08966471 | validation loss: 0.08780775\n",
      "Epoch 7/100\n",
      "train loss: 0.09024567 | validation loss: 0.08000571\n",
      "train loss: 0.09384529 | validation loss: 0.07815443\n",
      "train loss: 0.08168467 | validation loss: 0.07948535\n",
      "train loss: 0.08226484 | validation loss: 0.07661602\n",
      "train loss: 0.09626803 | validation loss: 0.07328852\n",
      "train loss: 0.08736855 | validation loss: 0.07726412\n",
      "train loss: 0.08615482 | validation loss: 0.0767172\n",
      "Epoch 8/100\n",
      "train loss: 0.07874033 | validation loss: 0.07082567\n",
      "train loss: 0.07976399 | validation loss: 0.06945609\n",
      "train loss: 0.07993707 | validation loss: 0.06935544\n",
      "train loss: 0.07409212 | validation loss: 0.06890319\n",
      "train loss: 0.06924911 | validation loss: 0.06534097\n",
      "train loss: 0.06995239 | validation loss: 0.06197417\n",
      "train loss: 0.06992938 | validation loss: 0.06553351\n",
      "Epoch 9/100\n",
      "train loss: 0.07802115 | validation loss: 0.06367307\n",
      "train loss: 0.06717305 | validation loss: 0.06215246\n",
      "train loss: 0.06557444 | validation loss: 0.05782421\n",
      "train loss: 0.06426976 | validation loss: 0.0627822\n",
      "train loss: 0.06476209 | validation loss: 0.06510022\n",
      "train loss: 0.0620648 | validation loss: 0.0574653\n",
      "train loss: 0.06234265 | validation loss: 0.06226574\n",
      "Epoch 10/100\n",
      "train loss: 0.06130884 | validation loss: 0.05541508\n",
      "train loss: 0.07196647 | validation loss: 0.06041794\n",
      "train loss: 0.05580196 | validation loss: 0.05553397\n",
      "train loss: 0.05874163 | validation loss: 0.0526314\n",
      "train loss: 0.06721282 | validation loss: 0.05066333\n",
      "train loss: 0.06578591 | validation loss: 0.04980087\n",
      "train loss: 0.0561975 | validation loss: 0.04920566\n",
      "Epoch 11/100\n",
      "train loss: 0.0625228 | validation loss: 0.05240318\n",
      "train loss: 0.05406594 | validation loss: 0.05097228\n",
      "train loss: 0.05354071 | validation loss: 0.0494361\n",
      "train loss: 0.05242674 | validation loss: 0.04961022\n",
      "train loss: 0.05189832 | validation loss: 0.04571341\n",
      "train loss: 0.05264554 | validation loss: 0.04509066\n",
      "train loss: 0.04951258 | validation loss: 0.04557856\n",
      "Epoch 12/100\n",
      "train loss: 0.05087562 | validation loss: 0.0443013\n",
      "train loss: 0.05067521 | validation loss: 0.04884089\n",
      "train loss: 0.06558397 | validation loss: 0.04767813\n",
      "train loss: 0.04910869 | validation loss: 0.04489876\n",
      "train loss: 0.04831584 | validation loss: 0.04127524\n",
      "train loss: 0.06422896 | validation loss: 0.03740942\n",
      "train loss: 0.04799487 | validation loss: 0.03973633\n",
      "Epoch 13/100\n",
      "train loss: 0.04752253 | validation loss: 0.04283498\n",
      "train loss: 0.04518607 | validation loss: 0.04031712\n",
      "train loss: 0.04232776 | validation loss: 0.03514196\n",
      "train loss: 0.04656534 | validation loss: 0.04009933\n",
      "train loss: 0.05277918 | validation loss: 0.04021811\n",
      "train loss: 0.04558486 | validation loss: 0.04039596\n",
      "train loss: 0.04579962 | validation loss: 0.03854151\n",
      "Epoch 14/100\n",
      "train loss: 0.03996449 | validation loss: 0.0334206\n",
      "train loss: 0.04311028 | validation loss: 0.03411392\n",
      "train loss: 0.04301513 | validation loss: 0.03794843\n",
      "train loss: 0.04126662 | validation loss: 0.03792474\n",
      "train loss: 0.04108432 | validation loss: 0.03646587\n",
      "train loss: 0.03865283 | validation loss: 0.03205463\n",
      "train loss: 0.04263171 | validation loss: 0.03445329\n",
      "Epoch 15/100\n",
      "train loss: 0.04865578 | validation loss: 0.03439313\n",
      "train loss: 0.04908953 | validation loss: 0.03398785\n",
      "train loss: 0.0539753 | validation loss: 0.03679654\n",
      "train loss: 0.04767784 | validation loss: 0.03919619\n",
      "train loss: 0.03911255 | validation loss: 0.03276751\n",
      "train loss: 0.04167698 | validation loss: 0.03067794\n",
      "train loss: 0.03753623 | validation loss: 0.03400264\n",
      "Epoch 16/100\n",
      "train loss: 0.03803955 | validation loss: 0.03006048\n",
      "train loss: 0.03672409 | validation loss: 0.02974942\n",
      "train loss: 0.03649151 | validation loss: 0.03219853\n",
      "train loss: 0.04693162 | validation loss: 0.02658754\n",
      "train loss: 0.03735328 | validation loss: 0.03318897\n",
      "train loss: 0.04667081 | validation loss: 0.0319871\n",
      "train loss: 0.03537457 | validation loss: 0.03158836\n",
      "Epoch 17/100\n",
      "train loss: 0.03439657 | validation loss: 0.02882474\n",
      "train loss: 0.04666888 | validation loss: 0.02800928\n",
      "train loss: 0.04201969 | validation loss: 0.02885333\n",
      "train loss: 0.05870662 | validation loss: 0.02527164\n",
      "train loss: 0.04197097 | validation loss: 0.02635905\n",
      "train loss: 0.03796764 | validation loss: 0.02743974\n",
      "train loss: 0.03409915 | validation loss: 0.02654672\n",
      "Epoch 18/100\n",
      "train loss: 0.03994492 | validation loss: 0.02713891\n",
      "train loss: 0.03364236 | validation loss: 0.03284578\n",
      "train loss: 0.03346687 | validation loss: 0.0269891\n",
      "train loss: 0.03362909 | validation loss: 0.02644472\n",
      "train loss: 0.04443929 | validation loss: 0.02538959\n",
      "train loss: 0.03220581 | validation loss: 0.02559613\n",
      "train loss: 0.03188547 | validation loss: 0.02453064\n",
      "Epoch 19/100\n",
      "train loss: 0.0333982 | validation loss: 0.02396966\n",
      "train loss: 0.03260691 | validation loss: 0.02664339\n",
      "train loss: 0.03063375 | validation loss: 0.02572902\n",
      "train loss: 0.02869499 | validation loss: 0.02608292\n",
      "train loss: 0.04623743 | validation loss: 0.03337777\n",
      "train loss: 0.02931615 | validation loss: 0.0243254\n",
      "train loss: 0.04054322 | validation loss: 0.02412475\n",
      "Epoch 20/100\n",
      "train loss: 0.05733826 | validation loss: 0.02488345\n",
      "train loss: 0.02630827 | validation loss: 0.02269448\n",
      "train loss: 0.04584745 | validation loss: 0.02375951\n",
      "train loss: 0.03058851 | validation loss: 0.02677617\n",
      "train loss: 0.05782958 | validation loss: 0.02391381\n",
      "train loss: 0.03023019 | validation loss: 0.02411665\n",
      "train loss: 0.02726446 | validation loss: 0.02304786\n",
      "Epoch 21/100\n",
      "train loss: 0.03088539 | validation loss: 0.02487893\n",
      "train loss: 0.02999443 | validation loss: 0.02404794\n",
      "train loss: 0.03360859 | validation loss: 0.02613913\n",
      "train loss: 0.02898876 | validation loss: 0.02292724\n",
      "train loss: 0.03251222 | validation loss: 0.02369902\n",
      "train loss: 0.03574492 | validation loss: 0.02316932\n",
      "train loss: 0.02712423 | validation loss: 0.02073741\n",
      "Epoch 22/100\n",
      "train loss: 0.02873651 | validation loss: 0.02161065\n",
      "train loss: 0.02512126 | validation loss: 0.02070654\n",
      "train loss: 0.02712124 | validation loss: 0.0218951\n",
      "train loss: 0.02864067 | validation loss: 0.02118441\n",
      "train loss: 0.02793357 | validation loss: 0.02359017\n",
      "train loss: 0.02785584 | validation loss: 0.0259146\n",
      "train loss: 0.02668049 | validation loss: 0.02154762\n",
      "Epoch 23/100\n",
      "train loss: 0.02648545 | validation loss: 0.02106006\n",
      "train loss: 0.03033201 | validation loss: 0.02012558\n",
      "train loss: 0.03597623 | validation loss: 0.02548317\n",
      "train loss: 0.02870711 | validation loss: 0.0213937\n",
      "train loss: 0.03029864 | validation loss: 0.02207426\n",
      "train loss: 0.02620175 | validation loss: 0.02398316\n",
      "train loss: 0.02497184 | validation loss: 0.0185994\n",
      "Epoch 24/100\n",
      "train loss: 0.02560335 | validation loss: 0.02284198\n",
      "train loss: 0.02463568 | validation loss: 0.02454858\n",
      "train loss: 0.03593681 | validation loss: 0.01879925\n",
      "train loss: 0.02123271 | validation loss: 0.01874142\n",
      "train loss: 0.02514696 | validation loss: 0.01919959\n",
      "train loss: 0.02626824 | validation loss: 0.01918652\n",
      "train loss: 0.03331172 | validation loss: 0.01969697\n",
      "Epoch 25/100\n",
      "train loss: 0.0434248 | validation loss: 0.01838301\n",
      "train loss: 0.02470093 | validation loss: 0.02444636\n",
      "train loss: 0.02358757 | validation loss: 0.01766919\n",
      "train loss: 0.02441776 | validation loss: 0.02466094\n",
      "train loss: 0.02553364 | validation loss: 0.02095405\n",
      "train loss: 0.02061826 | validation loss: 0.01979808\n",
      "train loss: 0.02369798 | validation loss: 0.01962155\n",
      "Epoch 26/100\n",
      "train loss: 0.03409521 | validation loss: 0.020581\n",
      "train loss: 0.02506629 | validation loss: 0.02350901\n",
      "train loss: 0.03012878 | validation loss: 0.02049817\n",
      "train loss: 0.02534867 | validation loss: 0.01865043\n",
      "train loss: 0.0295036 | validation loss: 0.01711735\n",
      "train loss: 0.0255478 | validation loss: 0.02005146\n",
      "train loss: 0.03204224 | validation loss: 0.01945422\n",
      "Epoch 27/100\n",
      "train loss: 0.02125029 | validation loss: 0.01955015\n",
      "train loss: 0.0204764 | validation loss: 0.01898119\n",
      "train loss: 0.02403057 | validation loss: 0.01886044\n",
      "train loss: 0.02163079 | validation loss: 0.01864812\n",
      "train loss: 0.02141763 | validation loss: 0.01854579\n",
      "train loss: 0.02118917 | validation loss: 0.018358\n",
      "train loss: 0.02221933 | validation loss: 0.01904278\n",
      "Epoch 28/100\n",
      "train loss: 0.02692962 | validation loss: 0.01902388\n",
      "train loss: 0.02059464 | validation loss: 0.01933821\n",
      "train loss: 0.02347434 | validation loss: 0.01699388\n",
      "train loss: 0.02104695 | validation loss: 0.01627135\n",
      "train loss: 0.02326475 | validation loss: 0.01864201\n",
      "train loss: 0.02024962 | validation loss: 0.01906337\n",
      "train loss: 0.02057008 | validation loss: 0.01990045\n",
      "Epoch 29/100\n",
      "train loss: 0.02200386 | validation loss: 0.01874408\n",
      "train loss: 0.02143053 | validation loss: 0.01893338\n",
      "train loss: 0.02641187 | validation loss: 0.01785208\n",
      "train loss: 0.02059286 | validation loss: 0.01880168\n",
      "train loss: 0.01915393 | validation loss: 0.01806554\n",
      "train loss: 0.02216312 | validation loss: 0.01760918\n",
      "train loss: 0.02022026 | validation loss: 0.01631513\n",
      "Epoch 30/100\n",
      "train loss: 0.02348233 | validation loss: 0.01735218\n",
      "train loss: 0.02055144 | validation loss: 0.01784861\n",
      "train loss: 0.01812882 | validation loss: 0.01628618\n",
      "train loss: 0.01921596 | validation loss: 0.01643761\n",
      "train loss: 0.01718367 | validation loss: 0.01602067\n",
      "train loss: 0.0174601 | validation loss: 0.01605313\n",
      "train loss: 0.01779544 | validation loss: 0.01606495\n",
      "Epoch 31/100\n",
      "train loss: 0.02161252 | validation loss: 0.01607213\n",
      "train loss: 0.01880733 | validation loss: 0.01552094\n",
      "train loss: 0.02274302 | validation loss: 0.01562888\n",
      "train loss: 0.01534443 | validation loss: 0.01503746\n",
      "train loss: 0.0157698 | validation loss: 0.01458768\n",
      "train loss: 0.01712351 | validation loss: 0.01422431\n",
      "train loss: 0.01754458 | validation loss: 0.01491511\n",
      "Epoch 32/100\n",
      "train loss: 0.01372063 | validation loss: 0.01416648\n",
      "train loss: 0.01851805 | validation loss: 0.01347367\n",
      "train loss: 0.01402111 | validation loss: 0.01434263\n",
      "train loss: 0.01536685 | validation loss: 0.0132299\n",
      "train loss: 0.01436309 | validation loss: 0.0131702\n",
      "train loss: 0.01744043 | validation loss: 0.01377839\n",
      "train loss: 0.01400101 | validation loss: 0.01349653\n",
      "Epoch 33/100\n",
      "train loss: 0.01240051 | validation loss: 0.01357135\n",
      "train loss: 0.01314427 | validation loss: 0.01286563\n",
      "train loss: 0.01247066 | validation loss: 0.01357255\n",
      "train loss: 0.01330083 | validation loss: 0.01239253\n",
      "train loss: 0.01198754 | validation loss: 0.01259602\n",
      "train loss: 0.01168189 | validation loss: 0.01275582\n",
      "train loss: 0.01873446 | validation loss: 0.01262559\n",
      "Epoch 34/100\n",
      "train loss: 0.01343788 | validation loss: 0.01196082\n",
      "train loss: 0.01164627 | validation loss: 0.01262419\n",
      "train loss: 0.01095772 | validation loss: 0.01215727\n",
      "train loss: 0.01459466 | validation loss: 0.01154821\n",
      "train loss: 0.01520303 | validation loss: 0.01188893\n",
      "train loss: 0.01299442 | validation loss: 0.01160473\n",
      "train loss: 0.01519367 | validation loss: 0.0122387\n",
      "Epoch 35/100\n",
      "train loss: 0.01280378 | validation loss: 0.01100451\n",
      "train loss: 0.01398062 | validation loss: 0.01139025\n",
      "train loss: 0.01113281 | validation loss: 0.01185969\n",
      "train loss: 0.01184665 | validation loss: 0.01207896\n",
      "train loss: 0.02118164 | validation loss: 0.01086297\n",
      "train loss: 0.01125605 | validation loss: 0.01157187\n",
      "train loss: 0.01190578 | validation loss: 0.01089754\n",
      "Epoch 36/100\n",
      "train loss: 0.01252524 | validation loss: 0.01089418\n",
      "train loss: 0.0110512 | validation loss: 0.01105736\n",
      "train loss: 0.0117157 | validation loss: 0.01111859\n",
      "train loss: 0.01225522 | validation loss: 0.01096504\n",
      "train loss: 0.00979337 | validation loss: 0.01054134\n",
      "train loss: 0.01339345 | validation loss: 0.0105445\n",
      "train loss: 0.01140993 | validation loss: 0.0105253\n",
      "Epoch 37/100\n",
      "train loss: 0.00919384 | validation loss: 0.01057377\n",
      "train loss: 0.00994166 | validation loss: 0.01056492\n",
      "train loss: 0.01134926 | validation loss: 0.0103099\n",
      "train loss: 0.01075531 | validation loss: 0.01025537\n",
      "train loss: 0.01153532 | validation loss: 0.01008682\n",
      "train loss: 0.00841937 | validation loss: 0.01006606\n",
      "train loss: 0.01236103 | validation loss: 0.00978414\n",
      "Epoch 38/100\n",
      "train loss: 0.01000023 | validation loss: 0.00980198\n",
      "train loss: 0.00979877 | validation loss: 0.00970917\n",
      "train loss: 0.00961567 | validation loss: 0.00974397\n",
      "train loss: 0.01004048 | validation loss: 0.00947511\n",
      "train loss: 0.01018679 | validation loss: 0.00951509\n",
      "train loss: 0.01143567 | validation loss: 0.00973438\n",
      "train loss: 0.00910075 | validation loss: 0.0097571\n",
      "Epoch 39/100\n",
      "train loss: 0.01029782 | validation loss: 0.009343\n",
      "train loss: 0.01148275 | validation loss: 0.00916211\n",
      "train loss: 0.01162983 | validation loss: 0.00939559\n",
      "train loss: 0.0080646 | validation loss: 0.00926922\n",
      "train loss: 0.00882997 | validation loss: 0.00924323\n",
      "train loss: 0.00811019 | validation loss: 0.00943155\n",
      "train loss: 0.00758485 | validation loss: 0.0095936\n",
      "Epoch 40/100\n",
      "train loss: 0.00746073 | validation loss: 0.00930019\n",
      "train loss: 0.0079404 | validation loss: 0.0088134\n",
      "train loss: 0.01056002 | validation loss: 0.00897443\n",
      "train loss: 0.0100014 | validation loss: 0.00908966\n",
      "train loss: 0.00796814 | validation loss: 0.00905775\n",
      "train loss: 0.00756686 | validation loss: 0.00895365\n",
      "train loss: 0.0118726 | validation loss: 0.00866095\n",
      "Epoch 41/100\n",
      "train loss: 0.0085693 | validation loss: 0.00860973\n",
      "train loss: 0.00891872 | validation loss: 0.00879481\n",
      "train loss: 0.01084675 | validation loss: 0.00876076\n",
      "train loss: 0.00800243 | validation loss: 0.00849889\n",
      "train loss: 0.00933146 | validation loss: 0.00853445\n",
      "train loss: 0.00896758 | validation loss: 0.00874951\n",
      "train loss: 0.0092762 | validation loss: 0.00872576\n",
      "Epoch 42/100\n",
      "train loss: 0.00816047 | validation loss: 0.0085259\n",
      "train loss: 0.00739114 | validation loss: 0.00858595\n",
      "train loss: 0.0077864 | validation loss: 0.0083179\n",
      "train loss: 0.0085942 | validation loss: 0.00841112\n",
      "train loss: 0.00905672 | validation loss: 0.00832949\n",
      "train loss: 0.01012486 | validation loss: 0.0084299\n",
      "train loss: 0.00775204 | validation loss: 0.00840453\n",
      "Epoch 43/100\n",
      "train loss: 0.00824308 | validation loss: 0.0086114\n",
      "train loss: 0.00762755 | validation loss: 0.00815489\n",
      "train loss: 0.01019068 | validation loss: 0.00828281\n",
      "train loss: 0.00784377 | validation loss: 0.00824703\n",
      "train loss: 0.01024356 | validation loss: 0.00816106\n",
      "train loss: 0.01410633 | validation loss: 0.00815043\n",
      "train loss: 0.00666544 | validation loss: 0.00795038\n",
      "Epoch 44/100\n",
      "train loss: 0.00956275 | validation loss: 0.00820108\n",
      "train loss: 0.00778265 | validation loss: 0.00804774\n",
      "train loss: 0.00726463 | validation loss: 0.00813095\n",
      "train loss: 0.00954013 | validation loss: 0.00794734\n",
      "train loss: 0.00700232 | validation loss: 0.00817509\n",
      "train loss: 0.00872794 | validation loss: 0.00811567\n",
      "train loss: 0.00885731 | validation loss: 0.00802364\n",
      "Epoch 45/100\n",
      "train loss: 0.00704155 | validation loss: 0.0080246\n",
      "train loss: 0.00700349 | validation loss: 0.00806972\n",
      "train loss: 0.00699196 | validation loss: 0.00786504\n",
      "train loss: 0.01015179 | validation loss: 0.00793121\n",
      "train loss: 0.00702039 | validation loss: 0.00790682\n",
      "train loss: 0.00734822 | validation loss: 0.00776828\n",
      "train loss: 0.00718287 | validation loss: 0.0075708\n",
      "Epoch 46/100\n",
      "train loss: 0.00677155 | validation loss: 0.00762296\n",
      "train loss: 0.00655717 | validation loss: 0.00778634\n",
      "train loss: 0.00776051 | validation loss: 0.00766684\n",
      "train loss: 0.00647075 | validation loss: 0.00779578\n",
      "train loss: 0.0092731 | validation loss: 0.00758441\n",
      "train loss: 0.0064448 | validation loss: 0.00768124\n",
      "train loss: 0.01003916 | validation loss: 0.00764797\n",
      "Epoch 47/100\n",
      "train loss: 0.00692191 | validation loss: 0.0075959\n",
      "train loss: 0.00767312 | validation loss: 0.00761398\n",
      "train loss: 0.00746806 | validation loss: 0.00762914\n",
      "train loss: 0.00806236 | validation loss: 0.00759988\n",
      "train loss: 0.00581372 | validation loss: 0.00753344\n",
      "train loss: 0.00648519 | validation loss: 0.00755545\n",
      "train loss: 0.00670522 | validation loss: 0.0073711\n",
      "Epoch 48/100\n",
      "train loss: 0.00611548 | validation loss: 0.00757731\n",
      "train loss: 0.00818673 | validation loss: 0.00764953\n",
      "train loss: 0.00688108 | validation loss: 0.00746378\n",
      "train loss: 0.00928556 | validation loss: 0.00748371\n",
      "train loss: 0.00715044 | validation loss: 0.00740876\n",
      "train loss: 0.00811258 | validation loss: 0.00732192\n",
      "train loss: 0.00570622 | validation loss: 0.00715135\n",
      "Epoch 49/100\n",
      "train loss: 0.00867402 | validation loss: 0.00744093\n",
      "train loss: 0.01076237 | validation loss: 0.00726008\n",
      "train loss: 0.00567986 | validation loss: 0.00738025\n",
      "train loss: 0.0059634 | validation loss: 0.00736572\n",
      "train loss: 0.00522046 | validation loss: 0.00737305\n",
      "train loss: 0.00848424 | validation loss: 0.00735973\n",
      "train loss: 0.00665704 | validation loss: 0.00728466\n",
      "Epoch 50/100\n",
      "train loss: 0.00642355 | validation loss: 0.00730126\n",
      "train loss: 0.0046687 | validation loss: 0.00711637\n",
      "train loss: 0.00469063 | validation loss: 0.00716815\n",
      "train loss: 0.00587809 | validation loss: 0.00728036\n",
      "train loss: 0.00606316 | validation loss: 0.00724006\n",
      "train loss: 0.00660015 | validation loss: 0.00715402\n",
      "train loss: 0.00682231 | validation loss: 0.0070656\n",
      "Epoch 51/100\n",
      "train loss: 0.01042421 | validation loss: 0.0070247\n",
      "train loss: 0.00623821 | validation loss: 0.00709149\n",
      "train loss: 0.00536622 | validation loss: 0.00687819\n",
      "train loss: 0.00530669 | validation loss: 0.00691743\n",
      "train loss: 0.00677499 | validation loss: 0.00703277\n",
      "train loss: 0.00714941 | validation loss: 0.00710669\n",
      "train loss: 0.00715683 | validation loss: 0.0071828\n",
      "Epoch 52/100\n",
      "train loss: 0.00634174 | validation loss: 0.00700917\n",
      "train loss: 0.00793072 | validation loss: 0.00694835\n",
      "train loss: 0.0051095 | validation loss: 0.00692\n",
      "train loss: 0.00975046 | validation loss: 0.00690584\n",
      "train loss: 0.00545039 | validation loss: 0.00700328\n",
      "train loss: 0.00455861 | validation loss: 0.00701707\n",
      "train loss: 0.00968141 | validation loss: 0.0070076\n",
      "Epoch 53/100\n",
      "train loss: 0.00697519 | validation loss: 0.00692943\n",
      "train loss: 0.00540157 | validation loss: 0.00678364\n",
      "train loss: 0.00437624 | validation loss: 0.00678979\n",
      "train loss: 0.00698853 | validation loss: 0.00684262\n",
      "train loss: 0.00563147 | validation loss: 0.00680346\n",
      "train loss: 0.0091218 | validation loss: 0.00677281\n",
      "train loss: 0.00639699 | validation loss: 0.00679852\n",
      "Epoch 54/100\n",
      "train loss: 0.00445907 | validation loss: 0.00687203\n",
      "train loss: 0.00663294 | validation loss: 0.0066869\n",
      "train loss: 0.00817195 | validation loss: 0.00657281\n",
      "train loss: 0.00696216 | validation loss: 0.00660484\n",
      "train loss: 0.00543361 | validation loss: 0.00673158\n",
      "train loss: 0.00440813 | validation loss: 0.00650461\n",
      "train loss: 0.00529153 | validation loss: 0.0065684\n",
      "Epoch 55/100\n",
      "train loss: 0.00679855 | validation loss: 0.00633381\n",
      "train loss: 0.00563572 | validation loss: 0.00654075\n",
      "train loss: 0.0060189 | validation loss: 0.00657912\n",
      "train loss: 0.00478179 | validation loss: 0.00652541\n",
      "train loss: 0.00510082 | validation loss: 0.0063936\n",
      "train loss: 0.00718779 | validation loss: 0.00634106\n",
      "train loss: 0.00571376 | validation loss: 0.00634511\n",
      "Epoch 56/100\n",
      "train loss: 0.00480927 | validation loss: 0.00623663\n",
      "train loss: 0.00502371 | validation loss: 0.00625467\n",
      "train loss: 0.00554454 | validation loss: 0.00630197\n",
      "train loss: 0.00948005 | validation loss: 0.00612896\n",
      "train loss: 0.00494227 | validation loss: 0.00611564\n",
      "train loss: 0.00553258 | validation loss: 0.00620085\n",
      "train loss: 0.00558825 | validation loss: 0.00615464\n",
      "Epoch 57/100\n",
      "train loss: 0.00672479 | validation loss: 0.00610654\n",
      "train loss: 0.0059166 | validation loss: 0.00606241\n",
      "train loss: 0.00536143 | validation loss: 0.00603961\n",
      "train loss: 0.00584398 | validation loss: 0.00607386\n",
      "train loss: 0.00895116 | validation loss: 0.00602589\n",
      "train loss: 0.00543487 | validation loss: 0.006206\n",
      "train loss: 0.00438207 | validation loss: 0.00611346\n",
      "Epoch 58/100\n",
      "train loss: 0.00414779 | validation loss: 0.00609904\n",
      "train loss: 0.00481534 | validation loss: 0.00602586\n",
      "train loss: 0.00650692 | validation loss: 0.00598941\n",
      "train loss: 0.00572172 | validation loss: 0.00599576\n",
      "train loss: 0.00509253 | validation loss: 0.0060354\n",
      "train loss: 0.00592884 | validation loss: 0.00610674\n",
      "train loss: 0.00577812 | validation loss: 0.00608449\n",
      "Epoch 59/100\n",
      "train loss: 0.00476695 | validation loss: 0.00600939\n",
      "train loss: 0.00782496 | validation loss: 0.00601508\n",
      "train loss: 0.00450738 | validation loss: 0.00597907\n",
      "train loss: 0.00450617 | validation loss: 0.00607829\n",
      "train loss: 0.0044824 | validation loss: 0.00595688\n",
      "train loss: 0.00757758 | validation loss: 0.00594443\n",
      "train loss: 0.003342 | validation loss: 0.00605266\n",
      "Epoch 60/100\n",
      "train loss: 0.00675656 | validation loss: 0.00603332\n",
      "train loss: 0.00548979 | validation loss: 0.00600273\n",
      "train loss: 0.00492665 | validation loss: 0.00592034\n",
      "train loss: 0.0054746 | validation loss: 0.00596469\n",
      "train loss: 0.00462909 | validation loss: 0.00597413\n",
      "train loss: 0.00532177 | validation loss: 0.0060824\n",
      "train loss: 0.00728876 | validation loss: 0.00593401\n",
      "Epoch 61/100\n",
      "train loss: 0.0075757 | validation loss: 0.00605197\n",
      "train loss: 0.00509305 | validation loss: 0.00598495\n",
      "train loss: 0.00492462 | validation loss: 0.00591133\n",
      "train loss: 0.00661313 | validation loss: 0.00594802\n",
      "train loss: 0.00483787 | validation loss: 0.00592258\n",
      "train loss: 0.00657988 | validation loss: 0.00596952\n",
      "train loss: 0.00523794 | validation loss: 0.0059767\n",
      "Epoch 62/100\n",
      "train loss: 0.00508464 | validation loss: 0.00598917\n",
      "train loss: 0.00597368 | validation loss: 0.00598831\n",
      "train loss: 0.00464002 | validation loss: 0.00599549\n",
      "train loss: 0.00491485 | validation loss: 0.00597053\n",
      "train loss: 0.00698799 | validation loss: 0.0060274\n",
      "train loss: 0.00859611 | validation loss: 0.00591968\n",
      "train loss: 0.00635616 | validation loss: 0.00586148\n",
      "Epoch 63/100\n",
      "train loss: 0.00492272 | validation loss: 0.00592968\n",
      "train loss: 0.00552545 | validation loss: 0.00588739\n",
      "train loss: 0.00711071 | validation loss: 0.00588157\n",
      "train loss: 0.00655947 | validation loss: 0.00592662\n",
      "train loss: 0.00600242 | validation loss: 0.00589803\n",
      "train loss: 0.00992379 | validation loss: 0.00587433\n",
      "train loss: 0.0061279 | validation loss: 0.00594215\n",
      "Epoch 64/100\n",
      "train loss: 0.00777726 | validation loss: 0.00589801\n",
      "train loss: 0.00698408 | validation loss: 0.00581479\n",
      "train loss: 0.00921911 | validation loss: 0.00586066\n",
      "train loss: 0.00564357 | validation loss: 0.00587989\n",
      "train loss: 0.00476865 | validation loss: 0.00583055\n",
      "train loss: 0.00656647 | validation loss: 0.00589803\n",
      "train loss: 0.00686415 | validation loss: 0.00592171\n",
      "Epoch 65/100\n",
      "train loss: 0.00539712 | validation loss: 0.00593562\n",
      "train loss: 0.00516136 | validation loss: 0.00586574\n",
      "train loss: 0.00567505 | validation loss: 0.00583752\n",
      "train loss: 0.0051917 | validation loss: 0.00586847\n",
      "train loss: 0.00524817 | validation loss: 0.00586932\n",
      "train loss: 0.00760917 | validation loss: 0.00591018\n",
      "train loss: 0.00632506 | validation loss: 0.00592982\n",
      "Epoch 66/100\n",
      "train loss: 0.00525531 | validation loss: 0.00583848\n",
      "train loss: 0.00766514 | validation loss: 0.00590093\n",
      "train loss: 0.00464777 | validation loss: 0.00592902\n",
      "train loss: 0.00608397 | validation loss: 0.00583026\n",
      "train loss: 0.0063232 | validation loss: 0.00585899\n",
      "train loss: 0.00472366 | validation loss: 0.0058199\n",
      "train loss: 0.00495808 | validation loss: 0.00592491\n",
      "Epoch 67/100\n",
      "train loss: 0.00581759 | validation loss: 0.00581512\n",
      "train loss: 0.00520452 | validation loss: 0.00587355\n",
      "train loss: 0.00733477 | validation loss: 0.00589923\n",
      "train loss: 0.0063279 | validation loss: 0.00583728\n",
      "train loss: 0.00624993 | validation loss: 0.00587072\n",
      "train loss: 0.00477043 | validation loss: 0.00587442\n",
      "train loss: 0.00661168 | validation loss: 0.00588093\n",
      "Epoch 68/100\n",
      "train loss: 0.00501205 | validation loss: 0.00584988\n",
      "train loss: 0.0068479 | validation loss: 0.00582006\n",
      "train loss: 0.00449464 | validation loss: 0.00588238\n",
      "train loss: 0.00706289 | validation loss: 0.0058262\n",
      "train loss: 0.00562652 | validation loss: 0.00580056\n",
      "train loss: 0.00710013 | validation loss: 0.00582114\n",
      "train loss: 0.00496761 | validation loss: 0.00581925\n",
      "Epoch 69/100\n",
      "train loss: 0.00530897 | validation loss: 0.00580577\n",
      "train loss: 0.00473216 | validation loss: 0.00581226\n",
      "train loss: 0.00506477 | validation loss: 0.00581855\n",
      "train loss: 0.00517982 | validation loss: 0.00579116\n",
      "train loss: 0.00517613 | validation loss: 0.00579336\n",
      "train loss: 0.00579487 | validation loss: 0.00581604\n",
      "train loss: 0.00655118 | validation loss: 0.00584498\n",
      "Epoch 70/100\n",
      "train loss: 0.0041406 | validation loss: 0.00585213\n",
      "train loss: 0.00539541 | validation loss: 0.00585267\n",
      "train loss: 0.00463388 | validation loss: 0.00581995\n",
      "train loss: 0.00607002 | validation loss: 0.00578673\n",
      "train loss: 0.00831629 | validation loss: 0.00574904\n",
      "train loss: 0.00566329 | validation loss: 0.00581277\n",
      "train loss: 0.00636085 | validation loss: 0.00576274\n",
      "Epoch 71/100\n",
      "train loss: 0.00534512 | validation loss: 0.00583605\n",
      "train loss: 0.00577179 | validation loss: 0.00587388\n",
      "train loss: 0.0050992 | validation loss: 0.00583457\n",
      "train loss: 0.00592938 | validation loss: 0.00579226\n",
      "train loss: 0.00449401 | validation loss: 0.00583041\n",
      "train loss: 0.00693124 | validation loss: 0.00586502\n",
      "train loss: 0.00635653 | validation loss: 0.00583044\n",
      "Epoch 72/100\n",
      "train loss: 0.00557496 | validation loss: 0.00587054\n",
      "train loss: 0.0058491 | validation loss: 0.00586363\n",
      "train loss: 0.00345787 | validation loss: 0.0058661\n",
      "train loss: 0.00649409 | validation loss: 0.00587329\n",
      "train loss: 0.0037631 | validation loss: 0.00577605\n",
      "train loss: 0.00491879 | validation loss: 0.00576239\n",
      "train loss: 0.00504622 | validation loss: 0.00577034\n",
      "Epoch 73/100\n",
      "train loss: 0.00589069 | validation loss: 0.00582768\n",
      "train loss: 0.0047418 | validation loss: 0.00583478\n",
      "train loss: 0.0061421 | validation loss: 0.00581402\n",
      "train loss: 0.0063486 | validation loss: 0.00578405\n",
      "train loss: 0.00446059 | validation loss: 0.00577991\n",
      "train loss: 0.00615047 | validation loss: 0.00579058\n",
      "train loss: 0.0077299 | validation loss: 0.0058326\n",
      "Epoch 74/100\n",
      "train loss: 0.00724256 | validation loss: 0.0057886\n",
      "train loss: 0.00548411 | validation loss: 0.00578542\n",
      "train loss: 0.00552274 | validation loss: 0.00577632\n",
      "train loss: 0.00445224 | validation loss: 0.00585267\n",
      "train loss: 0.00818371 | validation loss: 0.00589282\n",
      "train loss: 0.00558534 | validation loss: 0.00586254\n",
      "train loss: 0.00991135 | validation loss: 0.00586025\n",
      "Epoch 75/100\n",
      "train loss: 0.00368022 | validation loss: 0.00583404\n",
      "train loss: 0.00843758 | validation loss: 0.00581734\n",
      "train loss: 0.00646846 | validation loss: 0.00576116\n",
      "train loss: 0.0035174 | validation loss: 0.00580008\n",
      "train loss: 0.0058096 | validation loss: 0.00573953\n",
      "train loss: 0.00435867 | validation loss: 0.0057655\n",
      "train loss: 0.00544786 | validation loss: 0.0057939\n",
      "Epoch 76/100\n",
      "train loss: 0.00430252 | validation loss: 0.00583071\n",
      "train loss: 0.0035665 | validation loss: 0.0058284\n",
      "train loss: 0.00597351 | validation loss: 0.00583564\n",
      "train loss: 0.00659198 | validation loss: 0.005794\n",
      "train loss: 0.00710777 | validation loss: 0.00575994\n",
      "train loss: 0.00474426 | validation loss: 0.0058186\n",
      "train loss: 0.00501431 | validation loss: 0.00579875\n",
      "Epoch 77/100\n",
      "train loss: 0.00544196 | validation loss: 0.00576617\n",
      "train loss: 0.00485467 | validation loss: 0.00582351\n",
      "train loss: 0.00753465 | validation loss: 0.0058318\n",
      "train loss: 0.00850065 | validation loss: 0.00579403\n",
      "train loss: 0.00504894 | validation loss: 0.0058088\n",
      "train loss: 0.00491646 | validation loss: 0.00579005\n",
      "train loss: 0.00610631 | validation loss: 0.00573181\n",
      "Epoch 78/100\n",
      "train loss: 0.00566471 | validation loss: 0.00578731\n",
      "train loss: 0.00510533 | validation loss: 0.00577641\n",
      "train loss: 0.00647946 | validation loss: 0.00575987\n",
      "train loss: 0.00543798 | validation loss: 0.00579084\n",
      "train loss: 0.00595217 | validation loss: 0.00569324\n",
      "train loss: 0.00349195 | validation loss: 0.00575785\n",
      "train loss: 0.0053199 | validation loss: 0.00580616\n",
      "Epoch 79/100\n",
      "train loss: 0.00841029 | validation loss: 0.00577298\n",
      "train loss: 0.00693928 | validation loss: 0.00580561\n",
      "train loss: 0.00350579 | validation loss: 0.00581344\n",
      "train loss: 0.00641222 | validation loss: 0.00580336\n",
      "train loss: 0.00514657 | validation loss: 0.0058429\n",
      "train loss: 0.00636792 | validation loss: 0.00569342\n",
      "train loss: 0.0091241 | validation loss: 0.00574031\n",
      "Epoch 80/100\n",
      "train loss: 0.00782454 | validation loss: 0.00588208\n",
      "train loss: 0.00449593 | validation loss: 0.00578674\n",
      "train loss: 0.00664272 | validation loss: 0.00577741\n",
      "train loss: 0.00780681 | validation loss: 0.00579522\n",
      "train loss: 0.00440586 | validation loss: 0.00573892\n",
      "train loss: 0.00578103 | validation loss: 0.00574029\n",
      "train loss: 0.00554476 | validation loss: 0.0057733\n",
      "Epoch 81/100\n",
      "train loss: 0.00722136 | validation loss: 0.00576531\n",
      "train loss: 0.00480052 | validation loss: 0.00586364\n",
      "train loss: 0.00632141 | validation loss: 0.00575649\n",
      "train loss: 0.00585304 | validation loss: 0.00573935\n",
      "train loss: 0.00593723 | validation loss: 0.00574135\n",
      "train loss: 0.00544526 | validation loss: 0.00566638\n",
      "train loss: 0.00511576 | validation loss: 0.00570427\n",
      "Epoch 82/100\n",
      "train loss: 0.0044207 | validation loss: 0.00578654\n",
      "train loss: 0.00924204 | validation loss: 0.0057404\n",
      "train loss: 0.00416867 | validation loss: 0.00575881\n",
      "train loss: 0.00541484 | validation loss: 0.00581297\n",
      "train loss: 0.00580269 | validation loss: 0.00581329\n",
      "train loss: 0.0039513 | validation loss: 0.00575393\n",
      "train loss: 0.00874416 | validation loss: 0.0056957\n",
      "Epoch 83/100\n",
      "train loss: 0.00542132 | validation loss: 0.00575338\n",
      "train loss: 0.00527192 | validation loss: 0.00573168\n",
      "train loss: 0.00561699 | validation loss: 0.00571046\n",
      "train loss: 0.00479949 | validation loss: 0.00573602\n",
      "train loss: 0.00637392 | validation loss: 0.0056999\n",
      "train loss: 0.00496687 | validation loss: 0.00574136\n",
      "train loss: 0.00414453 | validation loss: 0.00573679\n",
      "Epoch 84/100\n",
      "train loss: 0.00532394 | validation loss: 0.00578685\n",
      "train loss: 0.00389195 | validation loss: 0.00577128\n",
      "train loss: 0.0053327 | validation loss: 0.00572391\n",
      "train loss: 0.00362501 | validation loss: 0.00580131\n",
      "train loss: 0.00442574 | validation loss: 0.00578184\n",
      "train loss: 0.00562712 | validation loss: 0.00578896\n",
      "train loss: 0.00785254 | validation loss: 0.00573826\n",
      "Epoch 85/100\n",
      "train loss: 0.006782 | validation loss: 0.00574741\n",
      "train loss: 0.00522003 | validation loss: 0.00569528\n",
      "train loss: 0.0065284 | validation loss: 0.00570073\n",
      "train loss: 0.00802324 | validation loss: 0.00573019\n",
      "train loss: 0.00436213 | validation loss: 0.00573935\n",
      "train loss: 0.00562234 | validation loss: 0.00573662\n",
      "train loss: 0.00455942 | validation loss: 0.00576336\n",
      "Epoch 86/100\n",
      "train loss: 0.00814864 | validation loss: 0.00573595\n",
      "train loss: 0.00561289 | validation loss: 0.00571302\n",
      "train loss: 0.00488358 | validation loss: 0.00573645\n",
      "train loss: 0.00529444 | validation loss: 0.00573412\n",
      "train loss: 0.0056085 | validation loss: 0.00570764\n",
      "train loss: 0.00501259 | validation loss: 0.00573917\n",
      "train loss: 0.00502182 | validation loss: 0.00574377\n",
      "Epoch 87/100\n",
      "train loss: 0.00878739 | validation loss: 0.00573136\n",
      "train loss: 0.00356399 | validation loss: 0.00572456\n",
      "train loss: 0.0056477 | validation loss: 0.0057325\n",
      "train loss: 0.00501392 | validation loss: 0.00567669\n",
      "train loss: 0.00575102 | validation loss: 0.00568781\n",
      "train loss: 0.00480932 | validation loss: 0.00569726\n",
      "train loss: 0.00566817 | validation loss: 0.00570731\n",
      "Epoch 88/100\n",
      "train loss: 0.0057779 | validation loss: 0.00572616\n",
      "train loss: 0.00461358 | validation loss: 0.00568503\n",
      "train loss: 0.0038346 | validation loss: 0.00564437\n",
      "train loss: 0.00668143 | validation loss: 0.00569828\n",
      "train loss: 0.00554151 | validation loss: 0.00573886\n",
      "train loss: 0.00399818 | validation loss: 0.00576941\n",
      "train loss: 0.00625565 | validation loss: 0.00570754\n",
      "Epoch 89/100\n",
      "train loss: 0.00564977 | validation loss: 0.0056824\n",
      "train loss: 0.00511416 | validation loss: 0.00567931\n",
      "train loss: 0.00451373 | validation loss: 0.00568659\n",
      "train loss: 0.00412651 | validation loss: 0.00568813\n",
      "train loss: 0.00555519 | validation loss: 0.00571835\n",
      "train loss: 0.00496946 | validation loss: 0.00571426\n",
      "train loss: 0.00514302 | validation loss: 0.00572511\n",
      "Epoch 90/100\n",
      "train loss: 0.00469001 | validation loss: 0.00570206\n",
      "train loss: 0.00422408 | validation loss: 0.00574012\n",
      "train loss: 0.0054209 | validation loss: 0.00570165\n",
      "train loss: 0.00563833 | validation loss: 0.00570184\n",
      "train loss: 0.00453345 | validation loss: 0.0057633\n",
      "train loss: 0.00659594 | validation loss: 0.00576043\n",
      "train loss: 0.00632435 | validation loss: 0.0057918\n",
      "Epoch 91/100\n",
      "train loss: 0.00545209 | validation loss: 0.00567756\n",
      "train loss: 0.0045798 | validation loss: 0.00570934\n",
      "train loss: 0.00663374 | validation loss: 0.00571507\n",
      "train loss: 0.0051421 | validation loss: 0.00574339\n",
      "train loss: 0.00588827 | validation loss: 0.00573698\n",
      "train loss: 0.00652969 | validation loss: 0.00569303\n",
      "train loss: 0.00583938 | validation loss: 0.0057234\n",
      "Epoch 92/100\n",
      "train loss: 0.00443914 | validation loss: 0.00571854\n",
      "train loss: 0.00526165 | validation loss: 0.00570882\n",
      "train loss: 0.00563648 | validation loss: 0.00571344\n",
      "train loss: 0.00672291 | validation loss: 0.00564211\n",
      "train loss: 0.00551899 | validation loss: 0.00571443\n",
      "train loss: 0.00566498 | validation loss: 0.0057367\n",
      "train loss: 0.00534288 | validation loss: 0.0057678\n",
      "Epoch 93/100\n",
      "train loss: 0.0052794 | validation loss: 0.00567555\n",
      "train loss: 0.00594474 | validation loss: 0.00564995\n",
      "train loss: 0.00531344 | validation loss: 0.0057317\n",
      "train loss: 0.00433514 | validation loss: 0.00567676\n",
      "train loss: 0.00905311 | validation loss: 0.00567855\n",
      "train loss: 0.0051492 | validation loss: 0.00567214\n",
      "train loss: 0.00473748 | validation loss: 0.00569626\n",
      "Epoch 94/100\n",
      "train loss: 0.00691032 | validation loss: 0.00572027\n",
      "train loss: 0.00490335 | validation loss: 0.00573804\n",
      "train loss: 0.00491386 | validation loss: 0.0057708\n",
      "train loss: 0.00679075 | validation loss: 0.00571876\n",
      "train loss: 0.00466466 | validation loss: 0.00570781\n",
      "train loss: 0.00504119 | validation loss: 0.00563794\n",
      "train loss: 0.00580325 | validation loss: 0.0056709\n",
      "Epoch 95/100\n",
      "train loss: 0.00572187 | validation loss: 0.00578207\n",
      "train loss: 0.00427859 | validation loss: 0.00579904\n",
      "train loss: 0.00358507 | validation loss: 0.00574679\n",
      "train loss: 0.00607007 | validation loss: 0.00568435\n",
      "train loss: 0.0040367 | validation loss: 0.00567534\n",
      "train loss: 0.00559263 | validation loss: 0.00566934\n",
      "train loss: 0.00545337 | validation loss: 0.00568104\n",
      "Epoch 96/100\n",
      "train loss: 0.00750507 | validation loss: 0.00570503\n",
      "train loss: 0.00417642 | validation loss: 0.00570444\n",
      "train loss: 0.00483011 | validation loss: 0.00565939\n",
      "train loss: 0.00522785 | validation loss: 0.0056428\n",
      "train loss: 0.00660748 | validation loss: 0.00568438\n",
      "train loss: 0.00547136 | validation loss: 0.0056567\n",
      "train loss: 0.00575691 | validation loss: 0.00572055\n",
      "Epoch 97/100\n",
      "train loss: 0.00682753 | validation loss: 0.00576049\n",
      "train loss: 0.00580635 | validation loss: 0.00563184\n",
      "train loss: 0.0048561 | validation loss: 0.00569771\n",
      "train loss: 0.00436061 | validation loss: 0.0056857\n",
      "train loss: 0.00434735 | validation loss: 0.00569917\n",
      "train loss: 0.00600187 | validation loss: 0.00575027\n",
      "train loss: 0.00483884 | validation loss: 0.00577593\n",
      "Epoch 98/100\n",
      "train loss: 0.00349597 | validation loss: 0.00569043\n",
      "train loss: 0.00692736 | validation loss: 0.00577703\n",
      "train loss: 0.00746463 | validation loss: 0.00572696\n",
      "train loss: 0.0050154 | validation loss: 0.00574202\n",
      "train loss: 0.00444872 | validation loss: 0.00575817\n",
      "train loss: 0.00440513 | validation loss: 0.00567647\n",
      "train loss: 0.00439608 | validation loss: 0.00564824\n",
      "Epoch 99/100\n",
      "train loss: 0.00778581 | validation loss: 0.00571518\n",
      "train loss: 0.00473007 | validation loss: 0.00573536\n",
      "train loss: 0.00864662 | validation loss: 0.00569188\n",
      "train loss: 0.00592536 | validation loss: 0.00569345\n",
      "train loss: 0.00556826 | validation loss: 0.00569801\n",
      "train loss: 0.00540374 | validation loss: 0.00571743\n",
      "train loss: 0.00443069 | validation loss: 0.00568856\n",
      "Epoch 100/100\n",
      "train loss: 0.00430619 | validation loss: 0.00565691\n",
      "train loss: 0.00464769 | validation loss: 0.00564861\n",
      "train loss: 0.0045313 | validation loss: 0.00568033\n",
      "train loss: 0.00483026 | validation loss: 0.00562937\n",
      "train loss: 0.00504076 | validation loss: 0.00566024\n",
      "train loss: 0.00649673 | validation loss: 0.00568224\n",
      "train loss: 0.00466917 | validation loss: 0.00563076\n"
     ]
    }
   ],
   "source": [
    "ae_1.train_loop(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "train loss: 0.00517184 | validation loss: 0.0056376\n",
      "train loss: 0.0064015 | validation loss: 0.00575637\n",
      "train loss: 0.0059176 | validation loss: 0.00572865\n",
      "train loss: 0.00527666 | validation loss: 0.00566695\n",
      "train loss: 0.00498553 | validation loss: 0.00575757\n",
      "train loss: 0.0050455 | validation loss: 0.00561518\n",
      "train loss: 0.00526225 | validation loss: 0.00576978\n",
      "Epoch 2/50\n",
      "train loss: 0.00474081 | validation loss: 0.00574069\n",
      "train loss: 0.00325228 | validation loss: 0.00573223\n",
      "train loss: 0.00597879 | validation loss: 0.00572235\n",
      "train loss: 0.00500081 | validation loss: 0.00568812\n",
      "train loss: 0.00392077 | validation loss: 0.0057294\n",
      "train loss: 0.00396918 | validation loss: 0.0057252\n",
      "train loss: 0.00479697 | validation loss: 0.00564089\n",
      "Epoch 3/50\n",
      "train loss: 0.00471961 | validation loss: 0.00564141\n",
      "train loss: 0.00387362 | validation loss: 0.00570826\n",
      "train loss: 0.00495496 | validation loss: 0.00574964\n",
      "train loss: 0.00633106 | validation loss: 0.00566638\n",
      "train loss: 0.00454911 | validation loss: 0.00568234\n",
      "train loss: 0.00616682 | validation loss: 0.00568367\n",
      "train loss: 0.00623809 | validation loss: 0.00569702\n",
      "Epoch 4/50\n",
      "train loss: 0.00397641 | validation loss: 0.00570429\n",
      "train loss: 0.00631451 | validation loss: 0.0056655\n",
      "train loss: 0.00415825 | validation loss: 0.0057396\n",
      "train loss: 0.00504768 | validation loss: 0.00573166\n",
      "train loss: 0.00541476 | validation loss: 0.00576085\n",
      "train loss: 0.00496619 | validation loss: 0.00568802\n",
      "train loss: 0.00534353 | validation loss: 0.0056733\n",
      "Epoch 5/50\n",
      "train loss: 0.00633937 | validation loss: 0.00570424\n",
      "train loss: 0.00513756 | validation loss: 0.00570913\n",
      "train loss: 0.00602542 | validation loss: 0.00563978\n",
      "train loss: 0.00475077 | validation loss: 0.00568785\n",
      "train loss: 0.00565536 | validation loss: 0.00564835\n",
      "train loss: 0.00761439 | validation loss: 0.00567806\n",
      "train loss: 0.00662179 | validation loss: 0.00568546\n",
      "Epoch 6/50\n",
      "train loss: 0.00575224 | validation loss: 0.00571522\n",
      "train loss: 0.00561286 | validation loss: 0.00568314\n",
      "train loss: 0.00485663 | validation loss: 0.00565598\n",
      "train loss: 0.00370504 | validation loss: 0.00576684\n",
      "train loss: 0.00410405 | validation loss: 0.00569819\n",
      "train loss: 0.00331398 | validation loss: 0.00574238\n",
      "train loss: 0.00440852 | validation loss: 0.00570348\n",
      "Epoch 7/50\n",
      "train loss: 0.00819014 | validation loss: 0.0056975\n",
      "train loss: 0.00535195 | validation loss: 0.00566615\n",
      "train loss: 0.00477088 | validation loss: 0.0057284\n",
      "train loss: 0.0045161 | validation loss: 0.00566012\n",
      "train loss: 0.00508536 | validation loss: 0.00570789\n",
      "train loss: 0.00483787 | validation loss: 0.00563289\n",
      "train loss: 0.00516829 | validation loss: 0.00564967\n",
      "Epoch 8/50\n",
      "train loss: 0.00535572 | validation loss: 0.0056716\n",
      "train loss: 0.00396456 | validation loss: 0.00558981\n",
      "train loss: 0.00532928 | validation loss: 0.00566464\n",
      "train loss: 0.00643861 | validation loss: 0.00575875\n",
      "train loss: 0.00521149 | validation loss: 0.00563613\n",
      "train loss: 0.00392684 | validation loss: 0.00570843\n",
      "train loss: 0.0071611 | validation loss: 0.00563833\n",
      "Epoch 9/50\n",
      "train loss: 0.00587111 | validation loss: 0.00574749\n",
      "train loss: 0.00472947 | validation loss: 0.00568732\n",
      "train loss: 0.00594803 | validation loss: 0.00566309\n",
      "train loss: 0.00613306 | validation loss: 0.00564795\n",
      "train loss: 0.00539331 | validation loss: 0.00564911\n",
      "train loss: 0.00792671 | validation loss: 0.00566165\n",
      "train loss: 0.00446777 | validation loss: 0.00565073\n",
      "Epoch 10/50\n",
      "train loss: 0.00408711 | validation loss: 0.00568662\n",
      "train loss: 0.00625322 | validation loss: 0.00561401\n",
      "train loss: 0.00676915 | validation loss: 0.00567451\n",
      "train loss: 0.00510449 | validation loss: 0.00568407\n",
      "train loss: 0.00476751 | validation loss: 0.00567405\n",
      "train loss: 0.00541529 | validation loss: 0.00564697\n",
      "train loss: 0.00529153 | validation loss: 0.00568628\n",
      "Epoch 11/50\n",
      "train loss: 0.00555265 | validation loss: 0.0056353\n",
      "train loss: 0.00680615 | validation loss: 0.00566479\n",
      "train loss: 0.00485141 | validation loss: 0.00568723\n",
      "train loss: 0.00691386 | validation loss: 0.00571367\n",
      "train loss: 0.00550264 | validation loss: 0.00566234\n",
      "train loss: 0.00587727 | validation loss: 0.00570151\n",
      "train loss: 0.00558441 | validation loss: 0.00571207\n",
      "Epoch 12/50\n",
      "train loss: 0.00495589 | validation loss: 0.00567854\n",
      "train loss: 0.0054049 | validation loss: 0.00570621\n",
      "train loss: 0.00658204 | validation loss: 0.00576012\n",
      "train loss: 0.00547717 | validation loss: 0.00565727\n",
      "train loss: 0.00415929 | validation loss: 0.00569637\n",
      "train loss: 0.00522622 | validation loss: 0.00560543\n",
      "train loss: 0.00552402 | validation loss: 0.00562469\n",
      "Epoch 13/50\n",
      "train loss: 0.00376418 | validation loss: 0.00567989\n",
      "train loss: 0.00420504 | validation loss: 0.00569275\n",
      "train loss: 0.0042784 | validation loss: 0.00568079\n",
      "train loss: 0.00429162 | validation loss: 0.00567228\n",
      "train loss: 0.00495805 | validation loss: 0.0056927\n",
      "train loss: 0.00486656 | validation loss: 0.00564618\n",
      "train loss: 0.00369791 | validation loss: 0.00567111\n",
      "Epoch 14/50\n",
      "train loss: 0.00498557 | validation loss: 0.00575519\n",
      "train loss: 0.00682346 | validation loss: 0.00569929\n",
      "train loss: 0.00672026 | validation loss: 0.00565619\n",
      "train loss: 0.00531265 | validation loss: 0.00563576\n",
      "train loss: 0.0046836 | validation loss: 0.00566767\n",
      "train loss: 0.00523255 | validation loss: 0.00566927\n",
      "train loss: 0.0060031 | validation loss: 0.00569663\n",
      "Epoch 15/50\n",
      "train loss: 0.00568403 | validation loss: 0.00569396\n",
      "train loss: 0.00444759 | validation loss: 0.00571295\n",
      "train loss: 0.00520508 | validation loss: 0.00574171\n",
      "train loss: 0.0039564 | validation loss: 0.00567711\n",
      "train loss: 0.00486982 | validation loss: 0.00565403\n",
      "train loss: 0.00436027 | validation loss: 0.0057005\n",
      "train loss: 0.00645643 | validation loss: 0.00574552\n",
      "Epoch 16/50\n",
      "train loss: 0.00393122 | validation loss: 0.005663\n",
      "train loss: 0.005737 | validation loss: 0.00565755\n",
      "train loss: 0.00469264 | validation loss: 0.0057217\n",
      "train loss: 0.0064217 | validation loss: 0.00565445\n",
      "train loss: 0.00838643 | validation loss: 0.0056814\n",
      "train loss: 0.00502584 | validation loss: 0.00573823\n",
      "train loss: 0.00572155 | validation loss: 0.00576568\n",
      "Epoch 17/50\n",
      "train loss: 0.00461477 | validation loss: 0.00570998\n",
      "train loss: 0.00535463 | validation loss: 0.00575346\n",
      "train loss: 0.0052356 | validation loss: 0.00571124\n",
      "train loss: 0.00499019 | validation loss: 0.00566358\n",
      "train loss: 0.00417681 | validation loss: 0.0057156\n",
      "train loss: 0.00503383 | validation loss: 0.00567334\n",
      "train loss: 0.00501431 | validation loss: 0.00566733\n",
      "Epoch 18/50\n",
      "train loss: 0.00543741 | validation loss: 0.0056887\n",
      "train loss: 0.0071443 | validation loss: 0.00572032\n",
      "train loss: 0.00596223 | validation loss: 0.00574678\n",
      "train loss: 0.00680708 | validation loss: 0.00572102\n",
      "train loss: 0.00552485 | validation loss: 0.00568252\n",
      "train loss: 0.00525374 | validation loss: 0.00572751\n",
      "train loss: 0.00649744 | validation loss: 0.00564374\n",
      "Epoch 19/50\n",
      "train loss: 0.00412328 | validation loss: 0.00565577\n",
      "train loss: 0.00540606 | validation loss: 0.00572092\n",
      "train loss: 0.00544001 | validation loss: 0.00567895\n",
      "train loss: 0.00463477 | validation loss: 0.00574427\n",
      "train loss: 0.00576404 | validation loss: 0.00569295\n",
      "train loss: 0.00744533 | validation loss: 0.00576672\n",
      "train loss: 0.00426259 | validation loss: 0.00572631\n",
      "Epoch 20/50\n",
      "train loss: 0.00427269 | validation loss: 0.00570695\n",
      "train loss: 0.00361048 | validation loss: 0.00565281\n",
      "train loss: 0.00623261 | validation loss: 0.00574355\n",
      "train loss: 0.00539364 | validation loss: 0.00578973\n",
      "train loss: 0.00427782 | validation loss: 0.00571589\n",
      "train loss: 0.00476731 | validation loss: 0.00580269\n",
      "train loss: 0.0061119 | validation loss: 0.00571164\n",
      "Epoch 21/50\n",
      "train loss: 0.00513672 | validation loss: 0.00572888\n",
      "train loss: 0.00497137 | validation loss: 0.00573256\n",
      "train loss: 0.00369587 | validation loss: 0.00568203\n",
      "train loss: 0.00488343 | validation loss: 0.00585503\n",
      "train loss: 0.0049519 | validation loss: 0.00572438\n",
      "train loss: 0.00339339 | validation loss: 0.00574768\n",
      "train loss: 0.00446421 | validation loss: 0.00569686\n",
      "Epoch 22/50\n",
      "train loss: 0.0052109 | validation loss: 0.0056936\n",
      "train loss: 0.00572887 | validation loss: 0.00572265\n",
      "train loss: 0.00413458 | validation loss: 0.00582021\n",
      "train loss: 0.00580705 | validation loss: 0.00565636\n",
      "train loss: 0.00542486 | validation loss: 0.00566094\n",
      "train loss: 0.00524281 | validation loss: 0.00569579\n",
      "train loss: 0.00424814 | validation loss: 0.00571903\n",
      "Epoch 23/50\n",
      "train loss: 0.00578578 | validation loss: 0.00563696\n",
      "train loss: 0.00408414 | validation loss: 0.00580646\n",
      "train loss: 0.00595481 | validation loss: 0.00570942\n",
      "train loss: 0.00408302 | validation loss: 0.00580483\n",
      "train loss: 0.0059514 | validation loss: 0.00571564\n",
      "train loss: 0.00562326 | validation loss: 0.00564396\n",
      "train loss: 0.00379838 | validation loss: 0.00569909\n",
      "Epoch 24/50\n",
      "train loss: 0.00583508 | validation loss: 0.00569538\n",
      "train loss: 0.00431697 | validation loss: 0.00569631\n",
      "train loss: 0.00482873 | validation loss: 0.0056467\n",
      "train loss: 0.00513 | validation loss: 0.00563967\n",
      "train loss: 0.00386227 | validation loss: 0.00569766\n",
      "train loss: 0.00521239 | validation loss: 0.00561274\n",
      "train loss: 0.00437033 | validation loss: 0.0056668\n",
      "Epoch 25/50\n",
      "train loss: 0.00608039 | validation loss: 0.00574137\n",
      "train loss: 0.00401627 | validation loss: 0.00564308\n",
      "train loss: 0.00638739 | validation loss: 0.00571523\n",
      "train loss: 0.00463333 | validation loss: 0.0056906\n",
      "train loss: 0.00601833 | validation loss: 0.00561427\n",
      "train loss: 0.00406072 | validation loss: 0.00568098\n",
      "train loss: 0.00687763 | validation loss: 0.00568487\n",
      "Epoch 26/50\n",
      "train loss: 0.00452458 | validation loss: 0.00562476\n",
      "train loss: 0.00464007 | validation loss: 0.0057772\n",
      "train loss: 0.00650032 | validation loss: 0.00574067\n",
      "train loss: 0.00458939 | validation loss: 0.00566181\n",
      "train loss: 0.00512391 | validation loss: 0.005694\n",
      "train loss: 0.00442685 | validation loss: 0.00582004\n",
      "train loss: 0.00437933 | validation loss: 0.00571765\n",
      "Epoch 27/50\n",
      "train loss: 0.00398435 | validation loss: 0.00570345\n",
      "train loss: 0.0043852 | validation loss: 0.00566645\n",
      "train loss: 0.00532535 | validation loss: 0.00568917\n",
      "train loss: 0.00462767 | validation loss: 0.00566298\n",
      "train loss: 0.00570347 | validation loss: 0.00577659\n",
      "train loss: 0.00462204 | validation loss: 0.00570451\n",
      "train loss: 0.00434607 | validation loss: 0.00570065\n",
      "Epoch 28/50\n",
      "train loss: 0.00490772 | validation loss: 0.0057542\n",
      "train loss: 0.00399275 | validation loss: 0.00569929\n",
      "train loss: 0.00439892 | validation loss: 0.00565034\n",
      "train loss: 0.00442847 | validation loss: 0.00566865\n",
      "train loss: 0.00485955 | validation loss: 0.00572349\n",
      "train loss: 0.00461754 | validation loss: 0.0056425\n",
      "train loss: 0.00410692 | validation loss: 0.00567587\n",
      "Epoch 29/50\n",
      "train loss: 0.00402033 | validation loss: 0.00575798\n",
      "train loss: 0.00567378 | validation loss: 0.00568956\n",
      "train loss: 0.00438319 | validation loss: 0.00567644\n",
      "train loss: 0.00368519 | validation loss: 0.00573124\n",
      "train loss: 0.00562906 | validation loss: 0.0057068\n",
      "train loss: 0.00431844 | validation loss: 0.00571289\n",
      "train loss: 0.00670727 | validation loss: 0.00569795\n",
      "Epoch 30/50\n",
      "train loss: 0.00441776 | validation loss: 0.00577081\n",
      "train loss: 0.00704723 | validation loss: 0.00572401\n",
      "train loss: 0.00470892 | validation loss: 0.00575132\n",
      "train loss: 0.00424051 | validation loss: 0.00569714\n",
      "train loss: 0.00594062 | validation loss: 0.00566634\n",
      "train loss: 0.00674714 | validation loss: 0.00574199\n",
      "train loss: 0.00618532 | validation loss: 0.00573587\n",
      "Epoch 31/50\n",
      "train loss: 0.00553004 | validation loss: 0.0057505\n",
      "train loss: 0.00510614 | validation loss: 0.00574833\n",
      "train loss: 0.0049113 | validation loss: 0.00573915\n",
      "train loss: 0.00444621 | validation loss: 0.0057034\n",
      "train loss: 0.00503956 | validation loss: 0.00569684\n",
      "train loss: 0.00488535 | validation loss: 0.00573039\n",
      "train loss: 0.0041281 | validation loss: 0.00575977\n",
      "Epoch 32/50\n",
      "train loss: 0.00452255 | validation loss: 0.00572119\n",
      "train loss: 0.00549791 | validation loss: 0.00571764\n",
      "train loss: 0.00447217 | validation loss: 0.0057352\n",
      "train loss: 0.0064784 | validation loss: 0.00576719\n",
      "train loss: 0.00669402 | validation loss: 0.00570781\n",
      "train loss: 0.00422325 | validation loss: 0.00568718\n",
      "train loss: 0.00470029 | validation loss: 0.00570976\n",
      "Epoch 33/50\n",
      "train loss: 0.00497498 | validation loss: 0.00566792\n",
      "train loss: 0.00760759 | validation loss: 0.00576538\n",
      "train loss: 0.00542925 | validation loss: 0.00569321\n",
      "train loss: 0.00599714 | validation loss: 0.005656\n",
      "train loss: 0.00625731 | validation loss: 0.00571772\n",
      "train loss: 0.00609403 | validation loss: 0.00567417\n",
      "train loss: 0.00828998 | validation loss: 0.00581069\n",
      "Epoch 34/50\n",
      "train loss: 0.00461525 | validation loss: 0.00567952\n",
      "train loss: 0.00473923 | validation loss: 0.00569317\n",
      "train loss: 0.00436262 | validation loss: 0.00572728\n",
      "train loss: 0.00531915 | validation loss: 0.00573243\n",
      "train loss: 0.00507724 | validation loss: 0.00576791\n",
      "train loss: 0.00570402 | validation loss: 0.00572723\n",
      "train loss: 0.00916936 | validation loss: 0.0057881\n",
      "Epoch 35/50\n",
      "train loss: 0.00264929 | validation loss: 0.00581249\n",
      "train loss: 0.00643211 | validation loss: 0.00574969\n",
      "train loss: 0.00366496 | validation loss: 0.00574494\n",
      "train loss: 0.00329546 | validation loss: 0.0057566\n",
      "train loss: 0.00702345 | validation loss: 0.00578444\n",
      "train loss: 0.00400271 | validation loss: 0.00574659\n",
      "train loss: 0.00453694 | validation loss: 0.00571936\n",
      "Epoch 36/50\n",
      "train loss: 0.00499365 | validation loss: 0.00576686\n",
      "train loss: 0.00604818 | validation loss: 0.00575411\n",
      "train loss: 0.00467406 | validation loss: 0.00570355\n",
      "train loss: 0.0044521 | validation loss: 0.00578659\n",
      "train loss: 0.00445187 | validation loss: 0.00574594\n",
      "train loss: 0.00475087 | validation loss: 0.00570348\n",
      "train loss: 0.00510515 | validation loss: 0.00576079\n",
      "Epoch 37/50\n",
      "train loss: 0.00698369 | validation loss: 0.00572793\n",
      "train loss: 0.00439773 | validation loss: 0.00566392\n",
      "train loss: 0.00588143 | validation loss: 0.00575405\n",
      "train loss: 0.00502284 | validation loss: 0.00578074\n",
      "train loss: 0.00909879 | validation loss: 0.0057956\n",
      "train loss: 0.00648717 | validation loss: 0.00574262\n",
      "train loss: 0.0049339 | validation loss: 0.005693\n",
      "Epoch 38/50\n",
      "train loss: 0.00404389 | validation loss: 0.00581424\n",
      "train loss: 0.00681515 | validation loss: 0.00577787\n",
      "train loss: 0.00492692 | validation loss: 0.00573418\n",
      "train loss: 0.00470787 | validation loss: 0.00570795\n",
      "train loss: 0.00388582 | validation loss: 0.00577004\n",
      "train loss: 0.00543074 | validation loss: 0.00579714\n",
      "train loss: 0.00413352 | validation loss: 0.00581145\n",
      "Epoch 39/50\n",
      "train loss: 0.00441631 | validation loss: 0.00579018\n",
      "train loss: 0.00448817 | validation loss: 0.00577277\n",
      "train loss: 0.00549461 | validation loss: 0.0057375\n",
      "train loss: 0.00463701 | validation loss: 0.00579339\n",
      "train loss: 0.00431783 | validation loss: 0.0056714\n",
      "train loss: 0.00464782 | validation loss: 0.00576547\n",
      "train loss: 0.00467023 | validation loss: 0.00574939\n",
      "Epoch 40/50\n",
      "train loss: 0.00555046 | validation loss: 0.00578033\n",
      "train loss: 0.00535218 | validation loss: 0.00576464\n",
      "train loss: 0.00561671 | validation loss: 0.00574794\n",
      "train loss: 0.0073349 | validation loss: 0.00581522\n",
      "train loss: 0.00682477 | validation loss: 0.00578673\n",
      "train loss: 0.00556481 | validation loss: 0.00589617\n",
      "train loss: 0.0060593 | validation loss: 0.00576416\n",
      "Epoch 41/50\n",
      "train loss: 0.005482 | validation loss: 0.00580162\n",
      "train loss: 0.00607863 | validation loss: 0.00576665\n",
      "train loss: 0.00503332 | validation loss: 0.00576865\n",
      "train loss: 0.00405686 | validation loss: 0.00582487\n",
      "train loss: 0.00713542 | validation loss: 0.00578958\n",
      "train loss: 0.00435719 | validation loss: 0.0057769\n",
      "train loss: 0.00457841 | validation loss: 0.00568108\n",
      "Epoch 42/50\n",
      "train loss: 0.00588391 | validation loss: 0.00571988\n",
      "train loss: 0.00639667 | validation loss: 0.00577043\n",
      "train loss: 0.00514076 | validation loss: 0.00580537\n",
      "train loss: 0.00490409 | validation loss: 0.00579518\n",
      "train loss: 0.00494541 | validation loss: 0.00574331\n",
      "train loss: 0.00888525 | validation loss: 0.00583045\n",
      "train loss: 0.003798 | validation loss: 0.00578658\n",
      "Epoch 43/50\n",
      "train loss: 0.00602653 | validation loss: 0.00575742\n",
      "train loss: 0.00482018 | validation loss: 0.00581451\n",
      "train loss: 0.00421702 | validation loss: 0.00576468\n",
      "train loss: 0.00509851 | validation loss: 0.00576782\n",
      "train loss: 0.0040206 | validation loss: 0.00571162\n",
      "train loss: 0.00492616 | validation loss: 0.00574645\n",
      "train loss: 0.00428484 | validation loss: 0.00574349\n",
      "Epoch 44/50\n",
      "train loss: 0.00403896 | validation loss: 0.00580394\n",
      "train loss: 0.00394684 | validation loss: 0.00576903\n",
      "train loss: 0.00568937 | validation loss: 0.00571121\n",
      "train loss: 0.00511928 | validation loss: 0.00575414\n",
      "train loss: 0.00483178 | validation loss: 0.00575066\n",
      "train loss: 0.00435258 | validation loss: 0.0057658\n",
      "train loss: 0.00504479 | validation loss: 0.00572686\n",
      "Epoch 45/50\n",
      "train loss: 0.00501265 | validation loss: 0.00575418\n",
      "train loss: 0.0045314 | validation loss: 0.00577282\n",
      "train loss: 0.00708349 | validation loss: 0.00575368\n",
      "train loss: 0.00472492 | validation loss: 0.00577116\n",
      "train loss: 0.00626441 | validation loss: 0.00579931\n",
      "train loss: 0.00542676 | validation loss: 0.00574788\n",
      "train loss: 0.0041303 | validation loss: 0.00574897\n",
      "Epoch 46/50\n",
      "train loss: 0.00575329 | validation loss: 0.00574237\n",
      "train loss: 0.00513758 | validation loss: 0.00581149\n",
      "train loss: 0.00672263 | validation loss: 0.00576845\n",
      "train loss: 0.00599584 | validation loss: 0.00579432\n",
      "train loss: 0.00555849 | validation loss: 0.00584995\n",
      "train loss: 0.00746321 | validation loss: 0.00575102\n",
      "train loss: 0.00654116 | validation loss: 0.00568558\n",
      "Epoch 47/50\n",
      "train loss: 0.00468875 | validation loss: 0.00573213\n",
      "train loss: 0.00438167 | validation loss: 0.00571907\n",
      "train loss: 0.0041672 | validation loss: 0.00574005\n",
      "train loss: 0.00372894 | validation loss: 0.00580528\n",
      "train loss: 0.00518482 | validation loss: 0.0057506\n",
      "train loss: 0.0040417 | validation loss: 0.00577859\n",
      "train loss: 0.00652346 | validation loss: 0.00576197\n",
      "Epoch 48/50\n",
      "train loss: 0.00445408 | validation loss: 0.00584334\n",
      "train loss: 0.00486709 | validation loss: 0.00573965\n",
      "train loss: 0.00496813 | validation loss: 0.00576745\n",
      "train loss: 0.00381665 | validation loss: 0.00576533\n",
      "train loss: 0.00360646 | validation loss: 0.00576835\n",
      "train loss: 0.0049595 | validation loss: 0.0057524\n",
      "train loss: 0.00493379 | validation loss: 0.00572652\n",
      "Epoch 49/50\n",
      "train loss: 0.00514838 | validation loss: 0.00582442\n",
      "train loss: 0.0055307 | validation loss: 0.00579719\n",
      "train loss: 0.00475064 | validation loss: 0.00572949\n",
      "train loss: 0.00622886 | validation loss: 0.0058109\n",
      "train loss: 0.00515064 | validation loss: 0.0058119\n",
      "train loss: 0.00447753 | validation loss: 0.00575391\n",
      "train loss: 0.00427912 | validation loss: 0.00579642\n",
      "Epoch 50/50\n",
      "train loss: 0.00314501 | validation loss: 0.00587537\n",
      "train loss: 0.00515852 | validation loss: 0.00585541\n",
      "train loss: 0.00434765 | validation loss: 0.00582419\n",
      "train loss: 0.00503788 | validation loss: 0.00583652\n",
      "train loss: 0.00423428 | validation loss: 0.00579021\n",
      "train loss: 0.00506885 | validation loss: 0.0056957\n",
      "train loss: 0.00539876 | validation loss: 0.00576782\n"
     ]
    }
   ],
   "source": [
    "ae_1.train_loop(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1 = pd.DataFrame(data=list(zip(ae_1.train_losses, ae_1.val_losses)), columns=['train_loss', 'validation_loss'])\n",
    "losses_1['epoch'] = (losses_1.index + 1) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7ee4cb1f60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 277.314375 \n",
       "L 392.14375 277.314375 \n",
       "L 392.14375 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "L 50.14375 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m084ca801b2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.071785\" xlink:href=\"#m084ca801b2\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(61.890535 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.593754\" xlink:href=\"#m084ca801b2\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 50 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(102.231254 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"152.115723\" xlink:href=\"#m084ca801b2\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 100 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(142.571973 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"195.637692\" xlink:href=\"#m084ca801b2\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(186.093942 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.159661\" xlink:href=\"#m084ca801b2\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 200 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(229.615911 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"282.68163\" xlink:href=\"#m084ca801b2\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(273.13788 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"326.203599\" xlink:href=\"#m084ca801b2\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 300 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(316.659849 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.725568\" xlink:href=\"#m084ca801b2\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 350 -->\n",
       "      <g transform=\"translate(360.181818 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- epoch -->\n",
       "     <defs>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(202.315625 268.034687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m4a7a787623\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4a7a787623\" y=\"232.085844\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.00 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 235.885063)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4a7a787623\" y=\"190.350982\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 194.1502)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4a7a787623\" y=\"148.61612\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 152.415338)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4a7a787623\" y=\"106.881257\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 110.680476)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4a7a787623\" y=\"65.146395\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 68.945614)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4a7a787623\" y=\"23.411533\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(20.878125 27.210752)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- MSE loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 24.515625 72.90625 \n",
       "L 43.109375 23.296875 \n",
       "L 61.8125 72.90625 \n",
       "L 76.515625 72.90625 \n",
       "L 76.515625 0 \n",
       "L 66.890625 0 \n",
       "L 66.890625 64.015625 \n",
       "L 48.09375 14.015625 \n",
       "L 38.1875 14.015625 \n",
       "L 19.390625 64.015625 \n",
       "L 19.390625 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-77\"/>\n",
       "      <path d=\"M 53.515625 70.515625 \n",
       "L 53.515625 60.890625 \n",
       "Q 47.90625 63.578125 42.921875 64.890625 \n",
       "Q 37.9375 66.21875 33.296875 66.21875 \n",
       "Q 25.25 66.21875 20.875 63.09375 \n",
       "Q 16.5 59.96875 16.5 54.203125 \n",
       "Q 16.5 49.359375 19.40625 46.890625 \n",
       "Q 22.3125 44.4375 30.421875 42.921875 \n",
       "L 36.375 41.703125 \n",
       "Q 47.40625 39.59375 52.65625 34.296875 \n",
       "Q 57.90625 29 57.90625 20.125 \n",
       "Q 57.90625 9.515625 50.796875 4.046875 \n",
       "Q 43.703125 -1.421875 29.984375 -1.421875 \n",
       "Q 24.8125 -1.421875 18.96875 -0.25 \n",
       "Q 13.140625 0.921875 6.890625 3.21875 \n",
       "L 6.890625 13.375 \n",
       "Q 12.890625 10.015625 18.65625 8.296875 \n",
       "Q 24.421875 6.59375 29.984375 6.59375 \n",
       "Q 38.421875 6.59375 43.015625 9.90625 \n",
       "Q 47.609375 13.234375 47.609375 19.390625 \n",
       "Q 47.609375 24.75 44.3125 27.78125 \n",
       "Q 41.015625 30.8125 33.5 32.328125 \n",
       "L 27.484375 33.5 \n",
       "Q 16.453125 35.6875 11.515625 40.375 \n",
       "Q 6.59375 45.0625 6.59375 53.421875 \n",
       "Q 6.59375 63.09375 13.40625 68.65625 \n",
       "Q 20.21875 74.21875 32.171875 74.21875 \n",
       "Q 37.3125 74.21875 42.625 73.28125 \n",
       "Q 47.953125 72.359375 53.515625 70.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-83\"/>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 55.90625 72.90625 \n",
       "L 55.90625 64.59375 \n",
       "L 19.671875 64.59375 \n",
       "L 19.671875 43.015625 \n",
       "L 54.390625 43.015625 \n",
       "L 54.390625 34.71875 \n",
       "L 19.671875 34.71875 \n",
       "L 19.671875 8.296875 \n",
       "L 56.78125 8.296875 \n",
       "L 56.78125 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-69\"/>\n",
       "      <path id=\"DejaVuSans-32\"/>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798438 152.932656)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n",
       "      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n",
       "      <use x=\"212.939453\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"244.726562\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"272.509766\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"333.691406\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"385.791016\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path clip-path=\"url(#pf8032414a8)\" d=\"M 65.361932 41.526125 \n",
       "L 65.942225 54.728844 \n",
       "L 66.232371 54.893187 \n",
       "L 66.522518 63.947846 \n",
       "L 66.812664 67.700254 \n",
       "L 67.102811 63.978904 \n",
       "L 67.392957 72.475634 \n",
       "L 67.683103 75.948772 \n",
       "L 67.97325 83.81139 \n",
       "L 68.263396 78.023075 \n",
       "L 68.553543 78.835784 \n",
       "L 69.133836 92.889383 \n",
       "L 69.714129 103.290327 \n",
       "L 70.004275 106.542206 \n",
       "L 70.294422 100.099294 \n",
       "L 70.584568 110.396754 \n",
       "L 70.874715 109.341356 \n",
       "L 71.164861 117.447323 \n",
       "L 71.455007 119.063984 \n",
       "L 71.745154 114.821334 \n",
       "L 72.325447 123.469832 \n",
       "L 72.615593 125.136655 \n",
       "L 72.90574 129.438559 \n",
       "L 73.195886 131.861386 \n",
       "L 73.486033 130.7473 \n",
       "L 73.776179 134.792702 \n",
       "L 74.066326 136.863859 \n",
       "L 74.356472 140.193438 \n",
       "L 74.646619 135.359052 \n",
       "L 75.226911 146.320015 \n",
       "L 75.517058 144.589634 \n",
       "L 75.807204 143.975031 \n",
       "L 76.097351 138.299821 \n",
       "L 76.387497 153.860627 \n",
       "L 76.677644 143.766297 \n",
       "L 77.257937 157.242955 \n",
       "L 77.548083 156.758031 \n",
       "L 77.83823 153.753437 \n",
       "L 78.128376 163.903874 \n",
       "L 78.418523 163.419608 \n",
       "L 78.708669 151.731186 \n",
       "L 78.998815 159.159558 \n",
       "L 79.288962 160.172654 \n",
       "L 79.579108 166.361509 \n",
       "L 79.869255 165.507064 \n",
       "L 80.159401 165.362591 \n",
       "L 80.739694 174.283804 \n",
       "L 81.029841 173.696776 \n",
       "L 81.319987 173.71598 \n",
       "L 81.610134 166.961802 \n",
       "L 81.90028 176.016685 \n",
       "L 82.480573 178.440053 \n",
       "L 82.770719 178.029103 \n",
       "L 83.060866 180.280523 \n",
       "L 83.351012 180.048605 \n",
       "L 83.641159 180.911525 \n",
       "L 83.931305 172.01563 \n",
       "L 84.221452 185.5081 \n",
       "L 84.511598 183.054368 \n",
       "L 84.801745 175.983488 \n",
       "L 85.091891 177.174528 \n",
       "L 85.382038 185.177947 \n",
       "L 85.672184 179.898236 \n",
       "L 85.96233 186.957157 \n",
       "L 86.252477 187.395557 \n",
       "L 86.542623 188.325385 \n",
       "L 86.83277 188.766457 \n",
       "L 87.122916 188.142755 \n",
       "L 87.413063 190.757827 \n",
       "L 87.703209 189.620102 \n",
       "L 87.993356 189.787389 \n",
       "L 88.283502 177.343088 \n",
       "L 88.573649 191.094958 \n",
       "L 88.863795 191.756744 \n",
       "L 89.153942 178.474109 \n",
       "L 89.444088 192.024658 \n",
       "L 89.734234 192.418916 \n",
       "L 90.314527 196.754981 \n",
       "L 90.604674 193.217887 \n",
       "L 90.89482 188.031205 \n",
       "L 91.184967 194.036283 \n",
       "L 91.475113 193.857024 \n",
       "L 91.76526 198.727591 \n",
       "L 92.055406 196.101814 \n",
       "L 92.345553 196.18123 \n",
       "L 92.635699 197.64071 \n",
       "L 92.925846 197.792876 \n",
       "L 93.215992 199.82243 \n",
       "L 93.506138 196.501275 \n",
       "L 93.796285 191.473 \n",
       "L 94.086431 191.110947 \n",
       "L 94.376578 187.032814 \n",
       "L 94.956871 199.438704 \n",
       "L 95.247017 197.298184 \n",
       "L 95.537164 200.754456 \n",
       "L 95.82731 200.334333 \n",
       "L 96.117457 201.432347 \n",
       "L 96.407603 201.626478 \n",
       "L 96.697749 192.912153 \n",
       "L 96.987896 200.907163 \n",
       "L 97.278042 193.129851 \n",
       "L 97.568189 202.558784 \n",
       "L 97.858335 203.375125 \n",
       "L 98.148482 193.131459 \n",
       "L 98.438628 197.012123 \n",
       "L 98.728775 183.083588 \n",
       "L 99.018921 197.052792 \n",
       "L 99.599214 203.623374 \n",
       "L 99.889361 198.743929 \n",
       "L 100.179507 204.00466 \n",
       "L 100.469653 204.151138 \n",
       "L 100.7598 204.015739 \n",
       "L 101.049946 194.992488 \n",
       "L 101.340093 205.203747 \n",
       "L 101.630239 205.471132 \n",
       "L 101.920386 204.208455 \n",
       "L 102.210532 204.868945 \n",
       "L 102.790825 208.134216 \n",
       "L 103.080972 193.491587 \n",
       "L 103.371118 207.615736 \n",
       "L 103.661265 198.244526 \n",
       "L 103.951411 184.225759 \n",
       "L 104.241557 210.126402 \n",
       "L 104.531704 193.817107 \n",
       "L 104.82185 206.553699 \n",
       "L 105.111997 183.815655 \n",
       "L 105.402143 206.852792 \n",
       "L 105.69229 209.328277 \n",
       "L 105.982436 206.305898 \n",
       "L 106.272583 207.049579 \n",
       "L 106.562729 204.032844 \n",
       "L 106.852876 207.889007 \n",
       "L 107.433168 202.249654 \n",
       "L 107.723315 209.445322 \n",
       "L 108.013461 208.099556 \n",
       "L 108.303608 211.117199 \n",
       "L 108.883901 208.179557 \n",
       "L 109.174047 208.76977 \n",
       "L 109.464194 208.834651 \n",
       "L 109.75434 209.815716 \n",
       "L 110.044487 209.978511 \n",
       "L 110.334633 206.7678 \n",
       "L 110.62478 202.056586 \n",
       "L 110.914926 208.124102 \n",
       "L 111.205072 206.795655 \n",
       "L 111.495219 210.215319 \n",
       "L 111.785365 211.241919 \n",
       "L 112.075512 210.714798 \n",
       "L 112.365658 211.522507 \n",
       "L 112.655805 202.089491 \n",
       "L 112.945951 214.362955 \n",
       "L 113.236098 211.095742 \n",
       "L 113.526244 210.159816 \n",
       "L 113.816391 204.280645 \n",
       "L 114.106537 195.83928 \n",
       "L 114.396684 211.468044 \n",
       "L 114.68683 212.397368 \n",
       "L 115.267123 210.772989 \n",
       "L 115.557269 214.875839 \n",
       "L 115.847416 212.305206 \n",
       "L 116.137562 203.626667 \n",
       "L 116.427709 211.163078 \n",
       "L 116.717855 206.937432 \n",
       "L 117.008002 210.927377 \n",
       "L 117.298148 207.459274 \n",
       "L 117.588295 210.761162 \n",
       "L 117.878441 205.340275 \n",
       "L 118.168588 214.348285 \n",
       "L 118.458734 214.994252 \n",
       "L 118.74888 212.027597 \n",
       "L 119.039027 214.030686 \n",
       "L 119.61932 214.399301 \n",
       "L 119.909466 213.539429 \n",
       "L 120.199613 209.607764 \n",
       "L 120.489759 214.895558 \n",
       "L 120.779906 212.491873 \n",
       "L 121.070052 214.518013 \n",
       "L 121.360199 212.666819 \n",
       "L 121.650345 215.18354 \n",
       "L 121.940491 214.916057 \n",
       "L 122.230638 213.719279 \n",
       "L 122.520784 214.197837 \n",
       "L 122.810931 210.039927 \n",
       "L 123.101077 214.89704 \n",
       "L 123.391224 216.09811 \n",
       "L 123.68137 213.586349 \n",
       "L 123.971517 215.208046 \n",
       "L 124.261663 212.485206 \n",
       "L 124.841956 216.953768 \n",
       "L 125.132103 216.046337 \n",
       "L 125.422249 217.742681 \n",
       "L 126.002542 217.232041 \n",
       "L 126.292688 214.045929 \n",
       "L 126.582835 216.38742 \n",
       "L 126.872981 213.102308 \n",
       "L 127.163128 219.277887 \n",
       "L 127.453274 218.922837 \n",
       "L 127.743421 217.792895 \n",
       "L 128.033567 217.441432 \n",
       "L 128.323714 220.633269 \n",
       "L 128.61386 216.628878 \n",
       "L 128.904007 220.382458 \n",
       "L 129.194153 219.259175 \n",
       "L 129.484299 220.09701 \n",
       "L 129.774446 217.528367 \n",
       "L 130.064592 220.399239 \n",
       "L 130.354739 221.735176 \n",
       "L 130.644885 221.114361 \n",
       "L 130.935032 221.676615 \n",
       "L 131.225178 220.983675 \n",
       "L 131.515325 222.07988 \n",
       "L 131.805471 222.335003 \n",
       "L 132.095618 216.448241 \n",
       "L 132.385764 220.869285 \n",
       "L 132.67591 222.364731 \n",
       "L 132.966057 222.939469 \n",
       "L 133.256203 219.903722 \n",
       "L 133.54635 219.39592 \n",
       "L 133.836496 221.239435 \n",
       "L 134.126643 219.403732 \n",
       "L 134.416789 221.398564 \n",
       "L 134.706936 220.41626 \n",
       "L 134.997082 222.793317 \n",
       "L 135.287229 222.197479 \n",
       "L 135.577375 214.405583 \n",
       "L 135.867522 222.690451 \n",
       "L 136.447814 221.631062 \n",
       "L 136.737961 222.86144 \n",
       "L 137.318254 221.856443 \n",
       "L 137.6084 223.911348 \n",
       "L 137.898547 220.906365 \n",
       "L 138.47884 224.411774 \n",
       "L 138.768986 223.787566 \n",
       "L 139.059133 222.612645 \n",
       "L 139.349279 223.10842 \n",
       "L 139.639426 222.457348 \n",
       "L 139.929572 225.058216 \n",
       "L 140.219718 221.76813 \n",
       "L 140.509865 223.73868 \n",
       "L 141.090158 224.059667 \n",
       "L 141.380304 223.705084 \n",
       "L 141.670451 223.582956 \n",
       "L 141.960597 222.540519 \n",
       "L 142.250744 224.489471 \n",
       "L 142.831037 222.501226 \n",
       "L 143.121183 222.378458 \n",
       "L 143.41133 225.354345 \n",
       "L 143.701476 224.715492 \n",
       "L 144.281769 225.754792 \n",
       "L 144.571915 225.858391 \n",
       "L 144.862062 225.458011 \n",
       "L 145.152208 223.271422 \n",
       "L 145.442355 223.737706 \n",
       "L 145.732501 225.434856 \n",
       "L 146.022648 225.769808 \n",
       "L 146.312794 222.175821 \n",
       "L 146.602941 224.933074 \n",
       "L 146.893087 224.641413 \n",
       "L 147.183233 223.032089 \n",
       "L 147.47338 225.406241 \n",
       "L 147.763526 224.296899 \n",
       "L 148.053673 224.600628 \n",
       "L 148.343819 224.343023 \n",
       "L 148.924112 225.916476 \n",
       "L 149.214259 225.586558 \n",
       "L 149.504405 224.912287 \n",
       "L 149.794552 224.526224 \n",
       "L 150.084698 223.634653 \n",
       "L 150.374845 225.615234 \n",
       "L 150.664991 225.205369 \n",
       "L 150.955137 225.71915 \n",
       "L 151.245284 223.579708 \n",
       "L 151.53543 225.538668 \n",
       "L 151.825577 223.535571 \n",
       "L 152.115723 220.311326 \n",
       "L 152.40587 226.522217 \n",
       "L 152.696016 224.103846 \n",
       "L 152.986163 225.589686 \n",
       "L 153.276309 226.02208 \n",
       "L 153.566456 224.122726 \n",
       "L 153.856602 226.241029 \n",
       "L 154.146749 224.80066 \n",
       "L 154.436895 224.692672 \n",
       "L 154.727041 226.208281 \n",
       "L 155.307334 226.249675 \n",
       "L 155.597481 223.612169 \n",
       "L 155.887627 226.22594 \n",
       "L 156.177774 225.952305 \n",
       "L 156.46792 226.090319 \n",
       "L 157.048213 226.61259 \n",
       "L 157.33836 225.608165 \n",
       "L 157.628506 226.684727 \n",
       "L 157.918653 224.345616 \n",
       "L 158.208799 226.70639 \n",
       "L 158.498945 223.706183 \n",
       "L 158.789092 226.308142 \n",
       "L 159.079238 225.681116 \n",
       "L 159.369385 225.852277 \n",
       "L 159.659531 225.356213 \n",
       "L 159.949678 227.23315 \n",
       "L 160.239824 226.672671 \n",
       "L 160.529971 226.489019 \n",
       "L 160.820117 226.981265 \n",
       "L 161.110264 225.252403 \n",
       "L 161.40041 226.342223 \n",
       "L 161.690556 224.335209 \n",
       "L 161.980703 226.117392 \n",
       "L 162.270849 225.314295 \n",
       "L 162.560996 227.322878 \n",
       "L 163.141289 223.102527 \n",
       "L 163.431435 227.344882 \n",
       "L 163.721582 227.108209 \n",
       "L 164.011728 227.728341 \n",
       "L 164.301875 225.004074 \n",
       "L 164.592021 226.529233 \n",
       "L 164.882168 226.724127 \n",
       "L 165.172314 228.18889 \n",
       "L 165.46246 228.170588 \n",
       "L 165.752607 227.17942 \n",
       "L 166.042753 227.024943 \n",
       "L 166.3329 226.576715 \n",
       "L 166.623046 226.391282 \n",
       "L 166.913193 223.384786 \n",
       "L 167.203339 226.878829 \n",
       "L 167.493486 227.606672 \n",
       "L 167.783632 227.656366 \n",
       "L 168.073779 226.430777 \n",
       "L 168.363925 226.11825 \n",
       "L 168.654072 226.112059 \n",
       "L 168.944218 226.792412 \n",
       "L 169.234364 225.466091 \n",
       "L 169.524511 227.820957 \n",
       "L 169.814657 223.947163 \n",
       "L 170.104804 227.536419 \n",
       "L 170.39495 228.280784 \n",
       "L 170.685097 224.004798 \n",
       "L 170.975243 226.263671 \n",
       "L 171.555536 228.43301 \n",
       "L 171.845683 226.252541 \n",
       "L 172.135829 227.385275 \n",
       "L 172.425975 224.471907 \n",
       "L 173.006268 228.363872 \n",
       "L 173.586561 225.264736 \n",
       "L 174.457001 228.40639 \n",
       "L 174.747147 227.66902 \n",
       "L 175.037294 226.411115 \n",
       "L 175.32744 227.381723 \n",
       "L 175.617587 227.061888 \n",
       "L 175.907733 228.094499 \n",
       "L 176.197879 227.828199 \n",
       "L 176.488026 226.086217 \n",
       "L 177.068319 228.071562 \n",
       "L 177.358465 227.892568 \n",
       "L 177.648612 227.457833 \n",
       "L 177.938758 224.17287 \n",
       "L 178.228905 227.960546 \n",
       "L 178.519051 227.467817 \n",
       "L 178.809198 227.421347 \n",
       "L 179.099344 226.472678 \n",
       "L 179.679637 227.610676 \n",
       "L 179.969783 227.20789 \n",
       "L 180.25993 224.614332 \n",
       "L 180.550076 227.549371 \n",
       "L 180.840223 228.428144 \n",
       "L 181.130369 228.623698 \n",
       "L 181.420516 228.066492 \n",
       "L 181.710662 226.654535 \n",
       "L 182.290955 227.835122 \n",
       "L 182.581102 227.137055 \n",
       "L 182.871248 227.262864 \n",
       "L 183.161395 228.106885 \n",
       "L 183.451541 225.554372 \n",
       "L 183.741687 228.323545 \n",
       "L 184.32198 228.344394 \n",
       "L 184.612127 225.760862 \n",
       "L 184.902273 229.296289 \n",
       "L 185.19242 226.446162 \n",
       "L 185.482566 227.50353 \n",
       "L 185.772713 227.973583 \n",
       "L 186.062859 227.516207 \n",
       "L 186.353006 228.221956 \n",
       "L 186.643152 227.64378 \n",
       "L 186.933298 226.001938 \n",
       "L 187.223445 225.762428 \n",
       "L 187.513591 227.834686 \n",
       "L 187.803738 227.97528 \n",
       "L 188.093884 226.565885 \n",
       "L 188.384031 228.04769 \n",
       "L 188.674177 226.593639 \n",
       "L 188.964324 227.71375 \n",
       "L 189.25447 227.841708 \n",
       "L 189.544617 227.099626 \n",
       "L 189.834763 228.212829 \n",
       "L 190.12491 227.983431 \n",
       "L 190.705202 224.910692 \n",
       "L 191.285495 227.976863 \n",
       "L 191.575642 227.473767 \n",
       "L 191.865788 226.150554 \n",
       "L 192.446081 227.075641 \n",
       "L 192.736228 223.802487 \n",
       "L 193.026374 226.970906 \n",
       "L 193.316521 225.594183 \n",
       "L 193.606667 226.256254 \n",
       "L 193.896814 224.390679 \n",
       "L 194.18696 227.37517 \n",
       "L 194.477106 228.105463 \n",
       "L 194.767253 226.604833 \n",
       "L 195.057399 226.356355 \n",
       "L 195.347546 227.580886 \n",
       "L 195.637692 227.777674 \n",
       "L 195.927839 227.348898 \n",
       "L 196.217985 227.752349 \n",
       "L 196.508132 227.705213 \n",
       "L 196.798278 225.734494 \n",
       "L 197.378571 227.699249 \n",
       "L 197.668718 225.687772 \n",
       "L 197.958864 228.206364 \n",
       "L 198.24901 227.007573 \n",
       "L 198.539157 226.807888 \n",
       "L 198.829303 228.143019 \n",
       "L 199.11945 227.947344 \n",
       "L 199.409596 227.229915 \n",
       "L 199.699743 227.741643 \n",
       "L 199.989889 225.963535 \n",
       "L 200.280036 226.803962 \n",
       "L 200.570182 226.869042 \n",
       "L 200.860329 228.103977 \n",
       "L 201.150475 226.56709 \n",
       "L 201.440621 227.902297 \n",
       "L 201.730768 226.369923 \n",
       "L 202.020914 228.334183 \n",
       "L 202.311061 226.190472 \n",
       "L 202.601207 227.389407 \n",
       "L 202.891354 226.159383 \n",
       "L 203.1815 227.939395 \n",
       "L 203.471647 227.654461 \n",
       "L 203.761793 228.135922 \n",
       "L 204.05194 227.858298 \n",
       "L 204.632233 227.765345 \n",
       "L 205.212525 226.617592 \n",
       "L 205.502672 228.629694 \n",
       "L 205.792818 227.582314 \n",
       "L 206.082965 228.217958 \n",
       "L 206.373111 227.019213 \n",
       "L 206.663258 225.144261 \n",
       "L 206.953404 227.35871 \n",
       "L 207.243551 226.776458 \n",
       "L 207.533697 227.624289 \n",
       "L 207.823844 227.26815 \n",
       "L 208.11399 227.829552 \n",
       "L 208.404137 227.136606 \n",
       "L 208.694283 228.334705 \n",
       "L 208.984429 226.300359 \n",
       "L 209.564722 227.432442 \n",
       "L 209.854869 227.20362 \n",
       "L 210.145015 229.199573 \n",
       "L 210.435162 226.665245 \n",
       "L 210.725308 228.944793 \n",
       "L 211.015455 227.980142 \n",
       "L 211.305601 227.87378 \n",
       "L 211.595748 227.1689 \n",
       "L 211.885894 228.127877 \n",
       "L 212.17604 226.959054 \n",
       "L 212.466187 226.786683 \n",
       "L 212.756333 228.362602 \n",
       "L 213.336626 225.63372 \n",
       "L 213.626773 226.040501 \n",
       "L 213.916919 227.508268 \n",
       "L 214.207066 227.47603 \n",
       "L 214.497212 228.369573 \n",
       "L 214.787359 225.25492 \n",
       "L 215.077505 227.423776 \n",
       "L 215.367652 223.812867 \n",
       "L 215.657798 229.013976 \n",
       "L 215.947944 225.043023 \n",
       "L 216.238091 226.686634 \n",
       "L 216.528237 229.14988 \n",
       "L 216.818384 227.236584 \n",
       "L 217.10853 228.447675 \n",
       "L 217.398677 227.538526 \n",
       "L 217.97897 229.108899 \n",
       "L 218.269116 227.099773 \n",
       "L 218.849409 226.153008 \n",
       "L 219.139556 228.125826 \n",
       "L 219.719848 227.543455 \n",
       "L 220.009995 228.033667 \n",
       "L 220.300141 225.796693 \n",
       "L 220.590288 224.990371 \n",
       "L 220.880434 227.871507 \n",
       "L 221.170581 227.98209 \n",
       "L 221.460727 226.988922 \n",
       "L 222.04102 227.82444 \n",
       "L 222.331167 226.677457 \n",
       "L 222.621313 227.54678 \n",
       "L 222.91146 227.11758 \n",
       "L 223.201606 229.17112 \n",
       "L 223.491752 227.64534 \n",
       "L 223.781899 225.0658 \n",
       "L 224.072045 226.29365 \n",
       "L 224.362192 229.159569 \n",
       "L 224.652338 226.733583 \n",
       "L 224.942485 227.790017 \n",
       "L 225.232631 226.770559 \n",
       "L 225.522778 224.469986 \n",
       "L 225.812924 225.55472 \n",
       "L 226.103071 228.333101 \n",
       "L 226.393217 226.541188 \n",
       "L 226.683363 225.569525 \n",
       "L 226.97351 228.408288 \n",
       "L 227.263656 227.26043 \n",
       "L 227.553803 227.457648 \n",
       "L 227.843949 226.058194 \n",
       "L 228.134096 228.078859 \n",
       "L 228.424242 226.809381 \n",
       "L 228.714389 227.200329 \n",
       "L 229.004535 227.130051 \n",
       "L 229.874975 228.395899 \n",
       "L 230.165121 224.37154 \n",
       "L 230.455267 228.606266 \n",
       "L 230.745414 227.566092 \n",
       "L 231.03556 227.242355 \n",
       "L 231.325707 228.787705 \n",
       "L 231.615853 224.78712 \n",
       "L 231.906 227.560687 \n",
       "L 232.196146 227.68539 \n",
       "L 232.486293 227.397359 \n",
       "L 232.776439 228.079722 \n",
       "L 233.066586 226.765553 \n",
       "L 233.356732 227.940013 \n",
       "L 233.646879 228.626414 \n",
       "L 233.937025 227.641963 \n",
       "L 234.227171 228.837242 \n",
       "L 234.517318 227.634657 \n",
       "L 234.807464 229.060056 \n",
       "L 235.097611 228.391693 \n",
       "L 235.387757 227.388902 \n",
       "L 235.677904 225.53135 \n",
       "L 235.96805 226.424928 \n",
       "L 236.258197 227.728698 \n",
       "L 236.83849 225.388865 \n",
       "L 237.128636 228.444788 \n",
       "L 237.418782 227.392891 \n",
       "L 237.708929 228.280109 \n",
       "L 237.999075 225.284196 \n",
       "L 238.289222 227.400783 \n",
       "L 238.579368 228.009533 \n",
       "L 239.159661 227.404448 \n",
       "L 239.449808 227.901849 \n",
       "L 239.739954 227.894145 \n",
       "L 240.030101 224.751031 \n",
       "L 240.320247 229.110994 \n",
       "L 240.610394 227.371726 \n",
       "L 240.90054 227.90074 \n",
       "L 241.190686 227.285487 \n",
       "L 241.480833 228.071519 \n",
       "L 241.770979 227.354639 \n",
       "L 242.061126 227.263048 \n",
       "L 242.641419 228.885116 \n",
       "L 242.931565 226.508875 \n",
       "L 243.511858 228.748573 \n",
       "L 243.802005 226.864272 \n",
       "L 244.672444 228.318249 \n",
       "L 244.96259 228.641457 \n",
       "L 245.252737 227.44894 \n",
       "L 245.542883 227.93785 \n",
       "L 245.83303 227.79298 \n",
       "L 246.413323 228.560014 \n",
       "L 246.703469 227.561035 \n",
       "L 246.993616 227.379548 \n",
       "L 247.283762 228.301782 \n",
       "L 247.573909 226.580229 \n",
       "L 247.864055 226.806928 \n",
       "L 248.444348 228.263102 \n",
       "L 248.734494 226.54868 \n",
       "L 249.024641 227.793751 \n",
       "L 249.604934 226.635532 \n",
       "L 249.89508 227.211729 \n",
       "L 250.185227 228.38051 \n",
       "L 250.475373 227.693957 \n",
       "L 250.76552 227.381088 \n",
       "L 251.055666 226.474249 \n",
       "L 251.345813 227.479157 \n",
       "L 251.635959 227.3573 \n",
       "L 251.926105 227.626158 \n",
       "L 252.216252 227.679143 \n",
       "L 252.506398 227.123789 \n",
       "L 252.796545 227.650726 \n",
       "L 253.086691 228.467316 \n",
       "L 253.376838 224.529235 \n",
       "L 253.666984 227.787824 \n",
       "L 253.957131 228.131486 \n",
       "L 254.247277 226.317818 \n",
       "L 254.537424 227.993033 \n",
       "L 254.82757 227.984259 \n",
       "L 255.117717 226.417625 \n",
       "L 255.407863 228.192264 \n",
       "L 255.698009 227.877979 \n",
       "L 255.988156 227.241887 \n",
       "L 256.278302 227.309816 \n",
       "L 256.568449 228.514519 \n",
       "L 256.858595 229.093398 \n",
       "L 257.148742 227.019174 \n",
       "L 257.438888 228.716422 \n",
       "L 257.729035 227.41769 \n",
       "L 258.019181 227.533935 \n",
       "L 258.309328 225.821381 \n",
       "L 258.599474 228.599798 \n",
       "L 259.179767 227.72217 \n",
       "L 259.469913 226.570598 \n",
       "L 259.76006 227.518915 \n",
       "L 260.050206 227.280567 \n",
       "L 260.340353 226.386923 \n",
       "L 260.920646 228.032469 \n",
       "L 261.210792 228.446051 \n",
       "L 261.500939 228.457126 \n",
       "L 261.791085 227.076098 \n",
       "L 262.371378 229.167768 \n",
       "L 262.661525 226.303594 \n",
       "L 262.951671 225.855137 \n",
       "L 263.241817 227.899506 \n",
       "L 263.531964 228.37251 \n",
       "L 264.112257 228.416445 \n",
       "L 264.402403 225.587048 \n",
       "L 264.69255 228.137669 \n",
       "L 264.982696 224.868537 \n",
       "L 265.272843 227.139966 \n",
       "L 265.562989 227.43803 \n",
       "L 265.853136 227.57536 \n",
       "L 266.143282 228.387557 \n",
       "L 266.433428 228.491481 \n",
       "L 266.723575 228.206429 \n",
       "L 267.013721 228.303576 \n",
       "L 267.594014 227.878338 \n",
       "L 267.884161 226.663041 \n",
       "L 268.174307 228.188501 \n",
       "L 268.464454 227.768926 \n",
       "L 268.7546 226.74253 \n",
       "L 269.62504 227.924434 \n",
       "L 269.915186 227.874377 \n",
       "L 270.205332 227.693457 \n",
       "L 270.495479 228.128704 \n",
       "L 270.785625 229.371176 \n",
       "L 271.075772 227.095364 \n",
       "L 271.656065 228.813184 \n",
       "L 271.946211 228.772778 \n",
       "L 272.236358 228.081827 \n",
       "L 272.526504 228.146395 \n",
       "L 272.816651 228.852547 \n",
       "L 273.396944 226.801322 \n",
       "L 273.68709 228.288718 \n",
       "L 273.977236 226.938419 \n",
       "L 274.267383 226.878926 \n",
       "L 274.557529 228.766743 \n",
       "L 274.847676 226.815136 \n",
       "L 275.137822 228.614961 \n",
       "L 275.427969 227.87256 \n",
       "L 275.718115 227.566155 \n",
       "L 276.008262 227.940582 \n",
       "L 276.298408 227.625615 \n",
       "L 276.588555 226.794393 \n",
       "L 276.878701 227.797538 \n",
       "L 277.168847 227.056442 \n",
       "L 277.458994 228.120388 \n",
       "L 277.74914 227.365329 \n",
       "L 278.039287 225.730131 \n",
       "L 278.61958 227.284462 \n",
       "L 278.909726 227.400806 \n",
       "L 279.199873 228.032028 \n",
       "L 279.490019 228.993258 \n",
       "L 279.780166 228.660208 \n",
       "L 280.070312 229.319673 \n",
       "L 280.360459 228.406065 \n",
       "L 280.650605 225.249553 \n",
       "L 280.940751 227.618589 \n",
       "L 281.230898 228.103602 \n",
       "L 281.521044 228.316266 \n",
       "L 281.811191 227.841112 \n",
       "L 282.101337 228.047684 \n",
       "L 282.68163 227.615437 \n",
       "L 282.971777 228.776639 \n",
       "L 283.55207 226.711555 \n",
       "L 284.132363 228.808121 \n",
       "L 284.422509 226.108497 \n",
       "L 285.002802 228.138169 \n",
       "L 285.292948 227.121044 \n",
       "L 285.583095 226.966596 \n",
       "L 285.873241 227.584061 \n",
       "L 286.163388 225.469444 \n",
       "L 286.453534 228.356612 \n",
       "L 286.743681 228.674343 \n",
       "L 287.033827 226.866299 \n",
       "L 287.323974 226.435656 \n",
       "L 287.61412 227.825141 \n",
       "L 287.904267 228.106413 \n",
       "L 288.194413 227.565719 \n",
       "L 288.484559 227.669018 \n",
       "L 288.774706 227.451063 \n",
       "L 289.064852 226.404769 \n",
       "L 289.354999 228.036387 \n",
       "L 289.645145 226.314863 \n",
       "L 289.935292 227.492807 \n",
       "L 290.225438 227.180105 \n",
       "L 290.515585 227.424555 \n",
       "L 290.805731 227.949176 \n",
       "L 291.095878 227.574386 \n",
       "L 291.386024 226.591831 \n",
       "L 291.966317 228.614096 \n",
       "L 292.256463 227.723529 \n",
       "L 292.54661 227.474958 \n",
       "L 292.836756 228.943891 \n",
       "L 293.126903 228.57591 \n",
       "L 293.707196 228.503639 \n",
       "L 293.997342 227.947372 \n",
       "L 294.287489 228.023736 \n",
       "L 294.577635 228.99921 \n",
       "L 294.867782 227.924399 \n",
       "L 295.157928 226.390324 \n",
       "L 295.448074 226.476458 \n",
       "L 295.738221 227.651386 \n",
       "L 296.028367 228.176455 \n",
       "L 296.60866 227.07507 \n",
       "L 296.898807 227.341402 \n",
       "L 297.188953 228.373456 \n",
       "L 297.4791 227.74118 \n",
       "L 297.769246 228.783451 \n",
       "L 298.059393 228.02102 \n",
       "L 298.349539 228.446343 \n",
       "L 298.639686 226.69668 \n",
       "L 298.929832 228.804468 \n",
       "L 299.219978 227.297185 \n",
       "L 299.510125 228.16891 \n",
       "L 300.090418 225.08571 \n",
       "L 300.380564 227.890792 \n",
       "L 300.670711 227.310085 \n",
       "L 300.960857 228.23391 \n",
       "L 301.251004 227.616353 \n",
       "L 301.831297 227.920543 \n",
       "L 302.121443 228.599471 \n",
       "L 302.41159 227.884116 \n",
       "L 302.701736 227.900415 \n",
       "L 302.991882 227.547255 \n",
       "L 303.282029 226.122518 \n",
       "L 303.572175 227.109185 \n",
       "L 303.862322 226.403989 \n",
       "L 304.152468 227.474269 \n",
       "L 304.442615 227.700561 \n",
       "L 304.732761 226.66245 \n",
       "L 305.022908 228.644156 \n",
       "L 305.313054 227.573422 \n",
       "L 305.603201 227.54508 \n",
       "L 305.893347 228.217216 \n",
       "L 306.183493 227.274617 \n",
       "L 306.47364 225.871249 \n",
       "L 306.763786 228.527873 \n",
       "L 307.053933 228.519442 \n",
       "L 307.344079 229.072183 \n",
       "L 307.634226 226.883499 \n",
       "L 308.214519 228.515157 \n",
       "L 308.504665 228.106586 \n",
       "L 308.794812 226.984257 \n",
       "L 309.084958 227.798235 \n",
       "L 309.375105 227.936257 \n",
       "L 309.665251 229.000915 \n",
       "L 309.955397 228.009658 \n",
       "L 310.245544 227.952502 \n",
       "L 310.53569 229.253391 \n",
       "L 311.115983 227.736321 \n",
       "L 311.40613 227.303969 \n",
       "L 311.696276 228.634722 \n",
       "L 311.986423 227.238714 \n",
       "L 312.276569 227.557727 \n",
       "L 312.566716 227.709688 \n",
       "L 312.856862 228.539937 \n",
       "L 313.147009 227.256467 \n",
       "L 313.437155 228.676824 \n",
       "L 313.727301 227.115382 \n",
       "L 314.017448 228.677761 \n",
       "L 314.307594 227.118226 \n",
       "L 314.597741 227.392121 \n",
       "L 314.887887 228.915348 \n",
       "L 315.178034 227.215317 \n",
       "L 315.46818 228.482483 \n",
       "L 316.048473 227.803845 \n",
       "L 316.33862 228.862018 \n",
       "L 316.628766 227.735077 \n",
       "L 316.918912 228.437942 \n",
       "L 317.209059 227.010562 \n",
       "L 317.499205 228.733476 \n",
       "L 317.789352 226.754309 \n",
       "L 318.079498 228.218416 \n",
       "L 318.369645 227.062356 \n",
       "L 318.659791 228.696373 \n",
       "L 318.949938 226.345103 \n",
       "L 319.240084 228.30919 \n",
       "L 319.530231 228.212788 \n",
       "L 319.820377 226.660047 \n",
       "L 320.110524 228.255096 \n",
       "L 320.40067 227.808933 \n",
       "L 320.690816 228.390763 \n",
       "L 320.980963 228.430426 \n",
       "L 321.271109 228.76012 \n",
       "L 321.561256 228.425527 \n",
       "L 321.851402 227.640786 \n",
       "L 322.141549 228.223138 \n",
       "L 322.431695 227.325171 \n",
       "L 322.721842 228.227839 \n",
       "L 323.011988 228.458191 \n",
       "L 323.302135 227.989385 \n",
       "L 323.592281 228.753105 \n",
       "L 323.882428 228.414077 \n",
       "L 324.172574 228.389414 \n",
       "L 324.46272 228.029588 \n",
       "L 324.752867 228.231594 \n",
       "L 325.043013 228.657807 \n",
       "L 325.33316 228.730086 \n",
       "L 325.623306 227.349952 \n",
       "L 325.913453 228.42721 \n",
       "L 326.203599 229.009823 \n",
       "L 326.493746 227.387284 \n",
       "L 326.783892 228.48125 \n",
       "L 327.074039 226.4873 \n",
       "L 327.364185 228.398349 \n",
       "L 327.654332 226.203541 \n",
       "L 327.944478 228.155321 \n",
       "L 328.234624 228.546301 \n",
       "L 328.524771 227.127224 \n",
       "L 328.814917 226.454024 \n",
       "L 329.685357 227.823765 \n",
       "L 329.975503 227.986396 \n",
       "L 330.26565 228.374603 \n",
       "L 330.555796 227.879335 \n",
       "L 330.845943 228.008055 \n",
       "L 331.136089 228.640128 \n",
       "L 331.426235 228.310881 \n",
       "L 331.716382 227.496757 \n",
       "L 332.006528 228.352933 \n",
       "L 332.296675 226.678339 \n",
       "L 332.586821 226.498361 \n",
       "L 332.876968 228.560707 \n",
       "L 333.457261 227.933243 \n",
       "L 333.747407 225.735811 \n",
       "L 334.037554 227.554067 \n",
       "L 334.3277 227.08005 \n",
       "L 334.617847 226.862888 \n",
       "L 334.907993 226.999171 \n",
       "L 335.198139 225.166219 \n",
       "L 335.488286 228.233506 \n",
       "L 335.778432 228.130022 \n",
       "L 336.068579 228.444377 \n",
       "L 336.358725 227.645964 \n",
       "L 336.648872 227.847886 \n",
       "L 336.939018 227.32471 \n",
       "L 337.229165 224.432202 \n",
       "L 337.519311 229.874489 \n",
       "L 337.809458 226.716979 \n",
       "L 338.099604 229.026712 \n",
       "L 338.389751 229.33513 \n",
       "L 338.679897 226.22339 \n",
       "L 338.970043 228.744793 \n",
       "L 339.550336 227.917659 \n",
       "L 339.840483 227.037442 \n",
       "L 340.130629 228.184422 \n",
       "L 340.420776 228.369688 \n",
       "L 340.710922 228.369878 \n",
       "L 341.291215 227.824592 \n",
       "L 341.581362 226.256579 \n",
       "L 341.871508 228.415071 \n",
       "L 342.161654 227.17663 \n",
       "L 342.451801 227.893297 \n",
       "L 342.741947 224.491108 \n",
       "L 343.032094 226.67102 \n",
       "L 343.32224 227.967531 \n",
       "L 343.612387 228.710421 \n",
       "L 343.902533 226.397261 \n",
       "L 344.19268 227.973358 \n",
       "L 344.482826 228.1562 \n",
       "L 344.772973 228.84236 \n",
       "L 345.063119 227.55282 \n",
       "L 345.353266 228.635609 \n",
       "L 345.643412 228.399566 \n",
       "L 345.933558 228.339581 \n",
       "L 346.223705 227.499512 \n",
       "L 346.513851 228.215341 \n",
       "L 346.803998 228.481766 \n",
       "L 347.094144 228.206323 \n",
       "L 347.384291 228.187616 \n",
       "L 347.674437 227.452889 \n",
       "L 347.964584 227.618398 \n",
       "L 348.25473 227.39759 \n",
       "L 348.544877 225.963421 \n",
       "L 348.835023 226.389228 \n",
       "L 349.12517 227.440912 \n",
       "L 349.415316 227.028165 \n",
       "L 349.705462 227.510035 \n",
       "L 349.995609 227.01203 \n",
       "L 350.575902 228.69959 \n",
       "L 350.866048 226.129931 \n",
       "L 351.156195 228.448912 \n",
       "L 351.446341 228.264257 \n",
       "L 351.736488 227.174563 \n",
       "L 352.026634 226.746562 \n",
       "L 352.316781 227.794865 \n",
       "L 352.606927 227.99241 \n",
       "L 352.897074 227.957924 \n",
       "L 353.18722 224.669352 \n",
       "L 353.477366 228.915666 \n",
       "L 353.767513 227.055517 \n",
       "L 354.057659 228.062456 \n",
       "L 354.347806 228.565913 \n",
       "L 354.637952 227.830134 \n",
       "L 354.928099 228.729864 \n",
       "L 355.218245 227.973994 \n",
       "L 355.508392 228.509298 \n",
       "L 356.088685 228.791426 \n",
       "L 356.378831 227.336945 \n",
       "L 356.668977 227.812795 \n",
       "L 356.959124 228.052773 \n",
       "L 357.24927 228.45276 \n",
       "L 357.539417 227.874971 \n",
       "L 357.829563 227.901798 \n",
       "L 358.11971 228.303494 \n",
       "L 358.409856 226.173278 \n",
       "L 358.700003 228.141969 \n",
       "L 358.990149 226.856961 \n",
       "L 359.280296 227.556146 \n",
       "L 359.570442 228.638295 \n",
       "L 359.860589 227.283585 \n",
       "L 360.150735 227.797521 \n",
       "L 360.440881 226.474485 \n",
       "L 361.021174 227.446183 \n",
       "L 361.311321 225.856325 \n",
       "L 361.601467 226.625957 \n",
       "L 361.891614 228.172158 \n",
       "L 362.762053 228.973311 \n",
       "L 363.0522 227.758085 \n",
       "L 363.342346 228.712244 \n",
       "L 363.632493 226.640727 \n",
       "L 363.922639 228.368038 \n",
       "L 364.212785 228.023294 \n",
       "L 364.502932 227.938961 \n",
       "L 364.793078 228.900096 \n",
       "L 365.083225 229.075545 \n",
       "L 365.373371 227.946164 \n",
       "L 365.663518 227.967625 \n",
       "L 365.953664 227.788508 \n",
       "L 366.243811 227.469384 \n",
       "L 366.533957 228.120494 \n",
       "L 366.824104 226.886632 \n",
       "L 367.404397 228.348463 \n",
       "L 367.694543 228.514073 \n",
       "L 367.984689 229.46071 \n",
       "L 368.274836 227.780046 \n",
       "L 368.564982 228.456873 \n",
       "L 368.855129 227.880742 \n",
       "L 369.145275 228.551501 \n",
       "L 369.435422 227.854886 \n",
       "L 369.725568 227.579517 \n",
       "L 369.725568 227.579517 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path clip-path=\"url(#pf8032414a8)\" d=\"M 65.361932 32.201761 \n",
       "L 65.652078 49.096514 \n",
       "L 65.942225 57.852475 \n",
       "L 66.232371 59.628031 \n",
       "L 66.522518 64.611598 \n",
       "L 66.812664 75.038587 \n",
       "L 67.683103 77.845934 \n",
       "L 67.97325 89.732704 \n",
       "L 68.263396 91.260495 \n",
       "L 68.843689 97.033089 \n",
       "L 69.133836 92.511032 \n",
       "L 69.423982 98.330554 \n",
       "L 69.714129 109.118468 \n",
       "L 70.004275 113.921734 \n",
       "L 70.294422 115.494315 \n",
       "L 70.584568 110.944286 \n",
       "L 71.164861 124.503376 \n",
       "L 71.455007 122.616352 \n",
       "L 71.745154 124.691749 \n",
       "L 72.0353 131.679326 \n",
       "L 72.325447 124.023595 \n",
       "L 72.615593 136.94899 \n",
       "L 72.90574 133.762727 \n",
       "L 73.195886 138.268838 \n",
       "L 73.486033 145.194921 \n",
       "L 73.776179 145.159815 \n",
       "L 74.066326 148.032697 \n",
       "L 74.356472 139.516004 \n",
       "L 74.646619 142.030885 \n",
       "L 74.936765 145.917038 \n",
       "L 75.517058 149.784521 \n",
       "L 75.807204 155.81412 \n",
       "L 76.097351 155.949675 \n",
       "L 76.387497 155.45151 \n",
       "L 76.677644 159.948354 \n",
       "L 76.96779 157.561604 \n",
       "L 77.257937 158.792955 \n",
       "L 77.548083 165.305296 \n",
       "L 77.83823 166.850557 \n",
       "L 78.128376 165.739642 \n",
       "L 78.708669 170.912122 \n",
       "L 78.998815 167.5937 \n",
       "L 79.288962 168.05021 \n",
       "L 79.579108 172.967849 \n",
       "L 79.869255 174.111034 \n",
       "L 80.159401 174.195047 \n",
       "L 80.449548 174.572545 \n",
       "L 81.029841 180.356174 \n",
       "L 81.319987 177.385203 \n",
       "L 81.90028 180.207357 \n",
       "L 82.190426 183.820133 \n",
       "L 82.480573 179.681716 \n",
       "L 82.770719 177.746867 \n",
       "L 83.060866 184.119716 \n",
       "L 83.351012 180.112803 \n",
       "L 83.641159 185.83103 \n",
       "L 83.931305 181.655154 \n",
       "L 84.221452 185.731791 \n",
       "L 84.801745 189.797299 \n",
       "L 85.382038 191.014018 \n",
       "L 85.672184 188.345052 \n",
       "L 86.252477 190.821665 \n",
       "L 86.542623 190.67633 \n",
       "L 86.83277 193.928984 \n",
       "L 87.122916 194.448791 \n",
       "L 87.413063 194.041544 \n",
       "L 87.703209 195.107667 \n",
       "L 87.993356 191.318489 \n",
       "L 88.283502 192.289042 \n",
       "L 88.863795 197.633514 \n",
       "L 89.153942 200.860303 \n",
       "L 89.734234 196.331605 \n",
       "L 90.024381 198.433253 \n",
       "L 90.314527 202.752947 \n",
       "L 90.604674 198.615043 \n",
       "L 91.184967 198.36745 \n",
       "L 91.475113 199.915348 \n",
       "L 91.76526 204.189758 \n",
       "L 92.055406 203.611045 \n",
       "L 92.345553 200.410391 \n",
       "L 92.635699 200.430171 \n",
       "L 92.925846 201.647884 \n",
       "L 93.215992 205.329936 \n",
       "L 93.506138 203.327776 \n",
       "L 93.796285 203.377995 \n",
       "L 94.086431 203.716279 \n",
       "L 94.666724 199.368889 \n",
       "L 94.956871 204.734892 \n",
       "L 95.247017 206.479051 \n",
       "L 95.537164 203.703938 \n",
       "L 95.82731 206.994441 \n",
       "L 96.117457 207.254085 \n",
       "L 96.407603 205.209817 \n",
       "L 96.697749 209.8933 \n",
       "L 96.987896 204.3831 \n",
       "L 97.278042 205.386296 \n",
       "L 97.568189 205.719126 \n",
       "L 97.858335 208.025917 \n",
       "L 98.148482 208.706575 \n",
       "L 98.438628 208.002048 \n",
       "L 98.728775 210.991678 \n",
       "L 99.309068 209.181972 \n",
       "L 99.599214 209.927371 \n",
       "L 99.889361 209.433066 \n",
       "L 100.179507 204.669559 \n",
       "L 100.469653 209.558116 \n",
       "L 100.7598 210.012507 \n",
       "L 101.049946 210.893224 \n",
       "L 101.340093 210.720827 \n",
       "L 101.630239 211.610186 \n",
       "L 101.920386 212.078432 \n",
       "L 102.210532 209.846681 \n",
       "L 102.500679 210.609906 \n",
       "L 102.790825 210.314499 \n",
       "L 103.080972 204.225508 \n",
       "L 103.371118 211.781499 \n",
       "L 103.661265 211.948981 \n",
       "L 103.951411 211.315694 \n",
       "L 104.241557 213.142826 \n",
       "L 104.531704 212.253843 \n",
       "L 104.82185 209.735848 \n",
       "L 105.111997 212.125054 \n",
       "L 105.402143 211.955739 \n",
       "L 105.69229 212.847858 \n",
       "L 105.982436 211.319468 \n",
       "L 106.272583 212.013092 \n",
       "L 106.562729 210.267588 \n",
       "L 106.852876 212.948537 \n",
       "L 107.143022 212.30434 \n",
       "L 107.433168 212.746476 \n",
       "L 107.723315 214.776387 \n",
       "L 108.013461 214.047496 \n",
       "L 108.303608 214.802152 \n",
       "L 108.593754 213.810068 \n",
       "L 108.883901 214.403271 \n",
       "L 109.464194 210.454996 \n",
       "L 109.75434 214.100101 \n",
       "L 110.044487 214.507069 \n",
       "L 110.334633 215.28708 \n",
       "L 110.62478 210.815113 \n",
       "L 110.914926 214.228585 \n",
       "L 111.205072 213.660523 \n",
       "L 111.495219 212.067163 \n",
       "L 111.785365 216.560977 \n",
       "L 112.075512 213.01971 \n",
       "L 112.365658 211.59521 \n",
       "L 112.655805 216.394161 \n",
       "L 112.945951 216.44243 \n",
       "L 113.236098 216.059995 \n",
       "L 113.526244 216.070911 \n",
       "L 113.816391 215.644839 \n",
       "L 114.106537 216.741594 \n",
       "L 114.396684 211.680536 \n",
       "L 114.68683 217.337422 \n",
       "L 114.976976 211.501426 \n",
       "L 115.267123 214.595556 \n",
       "L 115.557269 215.560443 \n",
       "L 115.847416 215.707794 \n",
       "L 116.137562 214.906937 \n",
       "L 116.427709 212.462937 \n",
       "L 116.717855 214.97608 \n",
       "L 117.298148 217.798041 \n",
       "L 117.588295 215.348946 \n",
       "L 117.878441 215.847461 \n",
       "L 118.168588 215.767384 \n",
       "L 118.458734 216.2423 \n",
       "L 119.61932 216.762475 \n",
       "L 119.909466 216.190891 \n",
       "L 120.199613 216.206667 \n",
       "L 120.489759 215.944294 \n",
       "L 120.779906 217.901102 \n",
       "L 121.070052 218.504195 \n",
       "L 121.360199 216.525413 \n",
       "L 121.650345 216.173701 \n",
       "L 121.940491 215.474996 \n",
       "L 122.230638 216.440212 \n",
       "L 122.520784 216.282207 \n",
       "L 122.810931 217.184758 \n",
       "L 123.101077 216.392137 \n",
       "L 123.971517 218.467648 \n",
       "L 124.261663 217.602028 \n",
       "L 124.55181 217.187658 \n",
       "L 124.841956 218.491812 \n",
       "L 125.132103 218.365419 \n",
       "L 125.422249 218.713436 \n",
       "L 126.292688 218.670485 \n",
       "L 126.582835 219.130556 \n",
       "L 126.872981 219.040461 \n",
       "L 127.453274 219.909548 \n",
       "L 127.743421 220.212854 \n",
       "L 128.033567 219.636241 \n",
       "L 128.61386 220.839409 \n",
       "L 128.904007 220.11409 \n",
       "L 129.194153 221.042881 \n",
       "L 129.484299 221.092717 \n",
       "L 129.774446 220.585063 \n",
       "L 130.064592 220.820326 \n",
       "L 130.354739 220.757876 \n",
       "L 130.644885 221.346941 \n",
       "L 130.935032 220.756874 \n",
       "L 131.225178 221.741837 \n",
       "L 131.805471 221.438594 \n",
       "L 132.095618 221.547299 \n",
       "L 132.385764 222.102183 \n",
       "L 132.67591 221.548469 \n",
       "L 133.256203 222.446588 \n",
       "L 133.54635 222.162186 \n",
       "L 133.836496 222.399405 \n",
       "L 134.126643 221.870236 \n",
       "L 134.416789 222.90041 \n",
       "L 135.287229 222.00357 \n",
       "L 135.577375 223.018555 \n",
       "L 135.867522 222.426835 \n",
       "L 136.157668 222.989699 \n",
       "L 136.447814 222.992498 \n",
       "L 137.028107 222.805185 \n",
       "L 137.318254 222.933359 \n",
       "L 137.6084 223.287018 \n",
       "L 138.768986 223.267332 \n",
       "L 139.059133 223.480203 \n",
       "L 139.929572 223.683733 \n",
       "L 140.219718 223.919052 \n",
       "L 141.090158 223.95258 \n",
       "L 141.380304 224.176992 \n",
       "L 141.670451 224.143628 \n",
       "L 141.960597 223.960581 \n",
       "L 142.250744 223.941617 \n",
       "L 142.54089 224.287271 \n",
       "L 142.831037 224.438252 \n",
       "L 143.121183 224.243372 \n",
       "L 143.701476 224.370541 \n",
       "L 144.281769 224.07809 \n",
       "L 144.571915 224.323005 \n",
       "L 144.862062 224.729326 \n",
       "L 145.442355 224.498734 \n",
       "L 146.022648 224.612254 \n",
       "L 146.312794 224.856573 \n",
       "L 146.602941 224.899327 \n",
       "L 146.893087 224.744837 \n",
       "L 147.183233 224.773263 \n",
       "L 147.47338 224.991847 \n",
       "L 147.763526 224.96216 \n",
       "L 148.053673 224.78265 \n",
       "L 148.343819 224.80248 \n",
       "L 148.633966 224.9693 \n",
       "L 148.924112 224.919172 \n",
       "L 149.214259 225.142913 \n",
       "L 149.504405 225.065102 \n",
       "L 149.794552 225.133241 \n",
       "L 150.664991 224.897929 \n",
       "L 150.955137 225.278977 \n",
       "L 151.245284 225.172209 \n",
       "L 152.115723 225.282702 \n",
       "L 152.40587 225.449682 \n",
       "L 152.696016 225.240425 \n",
       "L 152.986163 225.368419 \n",
       "L 153.276309 225.298966 \n",
       "L 153.566456 225.452219 \n",
       "L 153.856602 225.262115 \n",
       "L 155.597481 225.465685 \n",
       "L 156.177774 225.601684 \n",
       "L 156.46792 225.766521 \n",
       "L 157.628506 225.578724 \n",
       "L 157.918653 225.755161 \n",
       "L 158.498945 225.702108 \n",
       "L 159.079238 225.730477 \n",
       "L 160.239824 225.779335 \n",
       "L 160.529971 225.933207 \n",
       "L 161.110264 225.700802 \n",
       "L 161.40041 225.855851 \n",
       "L 161.980703 225.90177 \n",
       "L 162.560996 226.11663 \n",
       "L 162.851142 225.874922 \n",
       "L 163.141289 226.025876 \n",
       "L 163.721582 225.937701 \n",
       "L 164.882168 225.991504 \n",
       "L 165.172314 226.14583 \n",
       "L 166.042753 226.042583 \n",
       "L 166.913193 226.222345 \n",
       "L 167.203339 226.166594 \n",
       "L 167.493486 226.34464 \n",
       "L 168.363925 226.153906 \n",
       "L 168.654072 226.090378 \n",
       "L 169.234364 226.286074 \n",
       "L 169.814657 226.321556 \n",
       "L 170.39495 226.228716 \n",
       "L 170.975243 226.301872 \n",
       "L 171.26539 226.42356 \n",
       "L 173.006268 226.349781 \n",
       "L 173.586561 226.599541 \n",
       "L 174.166854 226.467012 \n",
       "L 174.457001 226.656463 \n",
       "L 174.747147 226.603217 \n",
       "L 175.037294 226.799028 \n",
       "L 175.32744 226.626296 \n",
       "L 175.907733 226.6391 \n",
       "L 176.488026 226.792977 \n",
       "L 177.648612 226.825611 \n",
       "L 177.938758 226.970016 \n",
       "L 180.25993 227.056051 \n",
       "L 180.550076 226.905713 \n",
       "L 181.420516 227.056074 \n",
       "L 182.290955 227.048109 \n",
       "L 182.871248 227.00714 \n",
       "L 183.741687 227.095127 \n",
       "L 184.031834 227.012314 \n",
       "L 184.612127 227.124047 \n",
       "L 185.19242 227.04985 \n",
       "L 186.353006 227.099251 \n",
       "L 186.643152 227.00888 \n",
       "L 186.933298 227.132741 \n",
       "L 187.223445 227.034285 \n",
       "L 188.093884 227.121051 \n",
       "L 188.964324 227.097111 \n",
       "L 190.705202 227.144707 \n",
       "L 191.285495 227.136357 \n",
       "L 192.155935 227.138913 \n",
       "L 193.896814 227.193963 \n",
       "L 195.637692 227.189725 \n",
       "L 196.508132 227.186735 \n",
       "L 197.088425 227.136238 \n",
       "L 197.378571 227.212484 \n",
       "L 197.958864 227.136909 \n",
       "L 198.539157 227.195358 \n",
       "L 198.829303 227.227991 \n",
       "L 199.11945 227.140334 \n",
       "L 199.409596 227.231978 \n",
       "L 199.989889 227.161776 \n",
       "L 200.570182 227.185574 \n",
       "L 206.082965 227.22795 \n",
       "L 207.243551 227.275701 \n",
       "L 207.823844 227.182936 \n",
       "L 208.694283 227.219216 \n",
       "L 209.854869 227.191491 \n",
       "L 215.947944 227.230123 \n",
       "L 216.818384 227.295077 \n",
       "L 218.559263 227.249607 \n",
       "L 219.719848 227.272838 \n",
       "L 220.300141 227.218054 \n",
       "L 221.460727 227.30152 \n",
       "L 222.04102 227.264287 \n",
       "L 223.781899 227.26715 \n",
       "L 224.942485 227.208793 \n",
       "L 225.232631 227.333565 \n",
       "L 226.683363 227.248589 \n",
       "L 227.263656 227.294442 \n",
       "L 228.424242 227.280918 \n",
       "L 230.455267 227.278981 \n",
       "L 231.03556 227.233507 \n",
       "L 231.906 227.28351 \n",
       "L 233.646879 227.297363 \n",
       "L 234.227171 227.268574 \n",
       "L 234.807464 227.243509 \n",
       "L 237.418782 227.297504 \n",
       "L 238.579368 227.297642 \n",
       "L 251.635959 227.297436 \n",
       "L 252.216252 227.348476 \n",
       "L 253.666984 227.351325 \n",
       "L 255.407863 227.321548 \n",
       "L 255.988156 227.35236 \n",
       "L 256.568449 227.245405 \n",
       "L 257.729035 227.35366 \n",
       "L 260.340353 227.277576 \n",
       "L 260.630499 227.384965 \n",
       "L 261.500939 227.328758 \n",
       "L 262.081232 227.264692 \n",
       "L 262.371378 227.336059 \n",
       "L 262.951671 227.305564 \n",
       "L 263.82211 227.347706 \n",
       "L 264.402403 227.315401 \n",
       "L 265.853136 227.313523 \n",
       "L 267.594014 227.36126 \n",
       "L 269.62504 227.280015 \n",
       "L 269.915186 227.398873 \n",
       "L 270.205332 227.269823 \n",
       "L 272.526504 227.376975 \n",
       "L 273.106797 227.286633 \n",
       "L 273.68709 227.34281 \n",
       "L 282.68163 227.351774 \n",
       "L 282.971777 227.420044 \n",
       "L 283.55207 227.279032 \n",
       "L 283.842216 227.381383 \n",
       "L 284.132363 227.321032 \n",
       "L 284.422509 227.379544 \n",
       "L 284.712655 227.288426 \n",
       "L 285.583095 227.371518 \n",
       "L 316.33862 227.330022 \n",
       "L 316.628766 227.400904 \n",
       "L 317.789352 227.315358 \n",
       "L 319.240084 227.390875 \n",
       "L 319.530231 227.263632 \n",
       "L 320.40067 227.33308 \n",
       "L 320.690816 227.227876 \n",
       "L 321.271109 227.32519 \n",
       "L 322.141549 227.358973 \n",
       "L 322.431695 227.264144 \n",
       "L 323.011988 227.327524 \n",
       "L 323.592281 227.328664 \n",
       "L 324.172574 227.354236 \n",
       "L 324.46272 227.308461 \n",
       "L 325.043013 227.348209 \n",
       "L 325.33316 227.279671 \n",
       "L 325.913453 227.347735 \n",
       "L 326.783892 227.31731 \n",
       "L 328.524771 227.356165 \n",
       "L 329.105064 227.298128 \n",
       "L 331.426235 227.310385 \n",
       "L 334.907993 227.34963 \n",
       "L 335.198139 227.23568 \n",
       "L 335.488286 227.345166 \n",
       "L 338.970043 227.289181 \n",
       "L 340.130629 227.325103 \n",
       "L 340.710922 227.289726 \n",
       "L 341.871508 227.358188 \n",
       "L 342.451801 227.26068 \n",
       "L 343.902533 227.263075 \n",
       "L 344.772973 227.269606 \n",
       "L 345.933558 227.267331 \n",
       "L 347.094144 227.273421 \n",
       "L 348.835023 227.25568 \n",
       "L 349.12517 227.164323 \n",
       "L 349.415316 227.274518 \n",
       "L 350.575902 227.223844 \n",
       "L 352.026634 227.269283 \n",
       "L 353.18722 227.21918 \n",
       "L 354.347806 227.274078 \n",
       "L 361.311321 227.285483 \n",
       "L 361.891614 227.301254 \n",
       "L 363.632493 227.276346 \n",
       "L 363.922639 227.20842 \n",
       "L 364.502932 227.271771 \n",
       "L 367.694543 227.247591 \n",
       "L 368.274836 227.198353 \n",
       "L 369.725568 227.271464 \n",
       "L 369.725568 227.271464 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- Autoencoder loss over time -->\n",
       "    <defs>\n",
       "     <path d=\"M 34.1875 63.1875 \n",
       "L 20.796875 26.90625 \n",
       "L 47.609375 26.90625 \n",
       "z\n",
       "M 28.609375 72.90625 \n",
       "L 39.796875 72.90625 \n",
       "L 67.578125 0 \n",
       "L 57.328125 0 \n",
       "L 50.6875 18.703125 \n",
       "L 17.828125 18.703125 \n",
       "L 11.1875 0 \n",
       "L 0.78125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-65\"/>\n",
       "     <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "     <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "     <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "     <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "     <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "     <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "     <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(134.935 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "     <use x=\"68.408203\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"131.787109\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"170.996094\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"232.177734\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"293.701172\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"357.080078\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"412.060547\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"473.242188\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"536.71875\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"598.242188\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"639.355469\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"671.142578\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"698.925781\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"760.107422\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"812.207031\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"864.306641\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"896.09375\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"957.275391\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "     <use x=\"1016.455078\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"1077.978516\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"1119.091797\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"1150.878906\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"1190.087891\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"1217.871094\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "     <use x=\"1315.283203\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 274.215625 59.674375 \n",
       "L 377.94375 59.674375 \n",
       "Q 379.94375 59.674375 379.94375 57.674375 \n",
       "L 379.94375 29.318125 \n",
       "Q 379.94375 27.318125 377.94375 27.318125 \n",
       "L 274.215625 27.318125 \n",
       "Q 272.215625 27.318125 272.215625 29.318125 \n",
       "L 272.215625 57.674375 \n",
       "Q 272.215625 59.674375 274.215625 59.674375 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 276.215625 35.416562 \n",
       "L 296.215625 35.416562 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\"/>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- train loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(304.215625 38.916562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 276.215625 50.094687 \n",
       "L 296.215625 50.094687 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_20\"/>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- validation loss -->\n",
       "     <g transform=\"translate(304.215625 53.594687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"492.333984\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"524.121094\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"551.904297\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"613.085938\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"665.185547\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pf8032414a8\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses_1['epoch'], losses_1['train_loss'], label='train loss')\n",
    "ax.plot(losses_1['epoch'], losses_1['validation_loss'], label='validation loss')\n",
    "ax.set_ylabel('MSE loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_title('Autoencoder loss over time')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_01 = ae_1.get_encoded_representations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('autoencoderhy_embeddings.pkl', 'wb') as fh:\n",
    "    pickle.dump(encoded_01, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1.to_csv('hyb_loss_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
